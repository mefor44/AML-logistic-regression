{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.datasets import make_classification\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from pathlib import Path\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif_(X, thresh = 5.0):\n",
    "    variables = list(range(X.shape[1]))\n",
    "    dropped = True\n",
    "    while dropped:\n",
    "        dropped = False\n",
    "        vif = [variance_inflation_factor(X.iloc[:, variables].values, ix)\n",
    "               for ix in range(X.iloc[:, variables].shape[1])]\n",
    "        maxloc = vif.index(max(vif))\n",
    "        if max(vif) > thresh:\n",
    "            print(f\"dropping {X.iloc[:, variables].columns[maxloc]} at index: {maxloc}\")\n",
    "            del variables[maxloc]\n",
    "            dropped = True\n",
    "\n",
    "    print('Remaining variables:')\n",
    "    print(X.columns[variables])\n",
    "    return X.iloc[:, variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the first data set (Credit risk data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"credit-data.csv\")\n",
    "df.drop([\"CUSTOMER_ID\", \"ASSESSMENT_YEAR\"],axis = 1, inplace = True)\n",
    "df.dropna(inplace = True) #drops 3 observations\n",
    "df = pd.get_dummies(df, drop_first = True)\n",
    "y1 = df.DEFAULT_FLAG.to_numpy()\n",
    "X1 = df.drop(\"DEFAULT_FLAG\", axis = 1)\n",
    "#X1 = calculate_vif_(X1, thresh = 100).to_numpy()\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, train_size = 0.8, random_state = 21, stratify = y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the second data set (Coke data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"coke-data.xls\")\n",
    "y2 = df.coke.to_numpy()\n",
    "X2 = df.drop(\"coke\", axis = 1)\n",
    "#X2 = calculate_vif_(X2, thresh = 10).to_numpy()\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, train_size = 0.8, random_state = 21, stratify = y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the third data set (Phone contract data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"phoneContract-data.xlsx\")\n",
    "df.drop(\"lp\", axis = 1, inplace = True)\n",
    "y3 = df.y_1_0.to_numpy()\n",
    "X3 = df.drop(\"y_1_0\", axis = 1)\n",
    "#X3 = calculate_vif_(X3, thresh = 10).to_numpy()\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, train_size = 0.8, random_state = 21, stratify = y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping 2 at index: 0\n",
      "dropping 22 at index: 19\n",
      "dropping 4 at index: 1\n",
      "dropping 24 at index: 19\n",
      "dropping 11 at index: 7\n",
      "dropping 26 at index: 19\n",
      "dropping 23 at index: 17\n",
      "dropping 31 at index: 22\n",
      "dropping 30 at index: 21\n",
      "dropping 8 at index: 4\n",
      "dropping 12 at index: 6\n",
      "dropping 29 at index: 18\n",
      "dropping 6 at index: 2\n",
      "dropping 7 at index: 2\n",
      "dropping 5 at index: 1\n",
      "dropping 27 at index: 13\n",
      "dropping 14 at index: 4\n",
      "dropping 10 at index: 2\n",
      "dropping 3 at index: 0\n",
      "dropping 19 at index: 6\n",
      "dropping 17 at index: 4\n",
      "dropping 25 at index: 7\n",
      "dropping 9 at index: 0\n",
      "dropping 20 at index: 4\n",
      "dropping 21 at index: 4\n",
      "dropping 16 at index: 2\n",
      "Remaining variables:\n",
      "Int64Index([13, 15, 18, 28], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "Cancer = pd.read_csv(\"wdbc.data\", header=None)\n",
    "Cancer.iloc[:,1] = (Cancer[1] == \"M\").astype(int)\n",
    "Cancer = Cancer.drop(columns=[0])\n",
    "Y = Cancer[1].to_numpy()\n",
    "X = Cancer.loc[:,2:]\n",
    "X = calculate_vif_(X)\n",
    "X = X.to_numpy()\n",
    "X_normalized = (X - X.mean(0)) / X.std(0)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_normalized, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping SepalLengthCm at index: 0\n",
      "dropping PetalLengthCm at index: 1\n",
      "Remaining variables:\n",
      "Index(['SepalWidthCm', 'PetalWidthCm'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# add a dataset to test multiclass\n",
    "df = pd.read_csv(\"Iris.csv\")\n",
    "df.drop(\"Id\", axis = 1, inplace = True)\n",
    "X = calculate_vif_(df.drop(\"Species\", axis = 1, inplace = False)).to_numpy()\n",
    "y = pd.get_dummies(df.Species, drop_first = False).to_numpy()\n",
    "X_multi_train, X_multi_test, Y_multi_train, Y_multi_test = train_test_split(X, y, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Cancer = pd.read_csv(\"wdbc.data\", header=None)\n",
    "Cancer.iloc[:,1] = (Cancer[1] == \"M\").astype(int)\n",
    "Cancer = Cancer.drop(columns=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.053e-01, 1.534e+02, 5.373e-02, 7.119e-01],\n",
       "       [7.339e-01, 7.408e+01, 1.860e-02, 2.416e-01],\n",
       "       [7.869e-01, 9.403e+01, 3.832e-02, 4.504e-01],\n",
       "       ...,\n",
       "       [1.075e+00, 4.855e+01, 4.730e-02, 3.403e-01],\n",
       "       [1.595e+00, 8.622e+01, 7.117e-02, 9.387e-01],\n",
       "       [1.428e+00, 1.915e+01, 0.000e+00, 0.000e+00]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"../Labs/lab2_logistic_regression/SAheart.data\")\n",
    "df = pd.read_csv(path, sep = \",\")\n",
    "df = df.drop(\"row.names\", axis = 1)\n",
    "df[\"famhist_present\"] = pd.get_dummies(df.famhist)[\"Present\"] # famhist can be: present | absent\n",
    "df.drop(\"famhist\", axis = 1, inplace = True)\n",
    "\n",
    "X = df.drop(\"chd\", axis = 1, inplace=False).to_numpy()\n",
    "y = df.chd.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.simple.test.10000.csv\")\n",
    "df[\"predict\"] = pd.get_dummies(df.cls, drop_first = True)\n",
    "df.drop(\"cls\", axis = 1, inplace = True)\n",
    "\n",
    "X = df.drop(\"predict\", axis = 1, inplace = False).to_numpy()\n",
    "y = df.predict.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiclass\n",
    "df = pd.read_csv(\"Iris.csv\")\n",
    "df.drop(\"Id\", axis = 1, inplace = True)\n",
    "X = df.drop(\"Species\", axis = 1, inplace = False).to_numpy()\n",
    "y = pd.get_dummies(df.Species, drop_first = False).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.normal(loc = 5, size = (4,1))\n",
    "X = np.random.normal(size = (4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -4.04974479,  -4.18799392,  -3.29439688,   6.8390388 ],\n",
       "       [  1.0370668 ,   1.07695354,   1.70801861,  -9.02168428],\n",
       "       [ -8.23941684,   1.76495676,   2.76678308,  -2.66326346],\n",
       "       [-15.97538503,  -3.11095196,  -7.4652715 ,   1.45880529]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.41000668],\n",
       "       [5.17813934],\n",
       "       [6.18265834],\n",
       "       [5.79558743]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_classification(n_samples=100, n_features=20, n_informative = 2,\n",
    "                         n_redundant = 1 , n_repeated=0, n_classes=2, n_clusters_per_class = 2)\n",
    "X = df[0]\n",
    "y = df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "class logit():\n",
    "    def __init__(self, exog, endog, add_intercept = True):\n",
    "        if len(endog.shape) > 1:\n",
    "            self.y = endog # T x 1\n",
    "        elif len(endog.shape) == 1:\n",
    "            self.y = endog.reshape(-1,1)\n",
    "        if add_intercept == True:\n",
    "            self.X = np.concatenate((np.ones(shape = (exog.shape[0], 1)), exog), axis = 1) # T x k\n",
    "        elif add_intercept == False:\n",
    "            self.X = exog\n",
    "        #self.b = np.linalg.inv(self.X.T @ self.X) @ self.X.T @ self.y # k x 1, initialized with LS estimator\n",
    "        if self.y.shape[1] == 1:\n",
    "            self.b = np.random.normal(size = (self.X.shape[1],1))\n",
    "            self.multiclass = 0\n",
    "        elif self.y.shape[1] > 1:# b - k x J (where J - number or classes)\n",
    "            self.b = np.random.normal(size = (self.X.shape[1], self.y.shape[1]))\n",
    "            self.multiclass = 1\n",
    "            \n",
    "\n",
    "    \n",
    "    def sigmoid(x): # returns output with the same dimensions as x \n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def cross_entrophy(y_true, y_pred): #returns a scalar value\n",
    "        # epsilon = 1e-5 ? why?\n",
    "        return -np.sum(y_true * np.log(y_pred, out=np.zeros_like(y_pred), where=(y_pred!=0)) + (1 - y_true) * np.log(1 - y_pred, out=np.zeros_like(1-y_pred), where=(1-y_pred!=0)))\n",
    "    \n",
    "    def cross_entrophy_der(X, y, b): # returns X.shape[0] x k vector \n",
    "        p_hat = logit.sigmoid(X @ b) # T x 1\n",
    "        return X * (p_hat - y)  # X.shape[0] x k\n",
    "    \"\"\"\n",
    "    def softmax(X, b):\n",
    "        return np.exp(X @ b) / np.sum(np.exp(X @ b))\n",
    "    \"\"\"\n",
    "    def softmax(x):\n",
    "        # Compute softmax values for each sets of scores in x.\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis = 0)\n",
    "    \n",
    "    def loglikelihood_multiclass(X, y, b):\n",
    "        # X - T x k\n",
    "        # y - T x J\n",
    "        # b - k x J\n",
    "        return np.sum(((X @ b) * y)[:,:-1]) - np.sum(np.log(1 + np.sum(np.exp(X @ b[:,:-1]), axis = 1)))\n",
    "                                                   \n",
    "    def loglikelihood_multiclass_der(X, y, b):\n",
    "        # X - T x k\n",
    "        # y - T x J\n",
    "        # b - k x J\n",
    "        return X.T @ (y - logit.softmax(X @ b))[:,:-1]\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X): # returns X.shape[0] x 1 vector\n",
    "        if self.multiclass == 1:\n",
    "            return logit.softmax(X @ self.b)\n",
    "        else:\n",
    "            return logit.sigmoid(X @ self.b)\n",
    "        \n",
    "    def predict_y(self, X, threshold = 0.5): # returns X.shape[0] x 1 vector\n",
    "        p_hat = 1 / (1 + np.exp( - X @ self.b))\n",
    "        if self.multiclass==0:\n",
    "            return np.array([1 if i[0] > threshold else 0 for i in p_hat]).reshape(-1,1)\n",
    "        else:\n",
    "            results = np.zeros(shape=p_hat.shape)\n",
    "            for i in range(0, len(p_hat)):\n",
    "                results[i][np.argmax(p_hat[i])] = 1\n",
    "            return results\n",
    "    \n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        y_hat = logit.predict_y(self, X)\n",
    "        truths = y_hat == y\n",
    "        ts = [np.prod(t) for t in truths]\n",
    "        correct_num = np.sum(ts)\n",
    "        return correct_num / X.shape[0]\n",
    "\n",
    "    def recall(self, X, y):\n",
    "        y_hat = logit.predict_y(self, X)\n",
    "        if not self.multiclass:\n",
    "            same = y[y_hat == y]\n",
    "            true_positives = np.sum(same[same == 1])\n",
    "            different = y[y_hat != y]\n",
    "            false_negatives = np.sum(different[different == 1])\n",
    "        else:\n",
    "            true_positives = 0\n",
    "            false_negatives = 0\n",
    "            truths = y_hat == y\n",
    "            ts = [np.prod(t) for t in truths]\n",
    "            same = y[ts]\n",
    "            fs = [not np.prod(t) for t in truths]\n",
    "            different = y[fs]\n",
    "            for i in range(0, self.y.shape[1]):\n",
    "                mask = np.zeros(shape=(self.y.shape[1]))\n",
    "                mask[i] = 1\n",
    "                true_positives += np.sum(same[same == mask])\n",
    "                false_negatives += np.sum(different[different == mask])\n",
    "        return true_positives/(false_negatives+true_positives)\n",
    "    \n",
    "    def precision(self, X, y):\n",
    "        y_hat = logit.predict_y(self, X)\n",
    "        if not self.multiclass:\n",
    "            same = y[y_hat == y]\n",
    "            true_positives = np.sum(same[same == 1])\n",
    "            different = y_hat[y_hat != y]\n",
    "            false_positives = np.sum(different[different == 1])\n",
    "        else:\n",
    "            true_positives = 0\n",
    "            false_positives = 0\n",
    "            truths = y_hat == y\n",
    "            ts = [np.prod(t) for t in truths]\n",
    "            same = y[ts]\n",
    "            fs = [not np.prod(t) for t in truths]\n",
    "            different = y_hat[fs]\n",
    "            for i in range(0, self.y.shape[1]):\n",
    "                mask = np.zeros(shape=(self.y.shape[1]))\n",
    "                mask[i] = 1\n",
    "                true_positives += np.sum(same[same == mask])\n",
    "                false_positives += np.sum(different[different == mask])\n",
    "        return true_positives/(false_positives+true_positives)\n",
    "    \n",
    "    def f_measure(self, X, y):\n",
    "        precision = self.precision(X, y)\n",
    "        recall = self.recall(X, y)\n",
    "        return 2 * ( precision * recall) / (precision + recall)\n",
    "    \n",
    "    \n",
    "    def fit(self, stop_condition = 0.001, lr = 0.001, method = \"GD\", verbose = False,\n",
    "           max_iter = 300):\n",
    "        # list with losses obtained when running the learing process\n",
    "        loss = []\n",
    "        betas = []\n",
    "        # p - vector of probabilities, shape T x 1\n",
    "        p = logit.sigmoid(self.X @ self.b)\n",
    "        loss.append(logit.cross_entrophy(self.y, p))\n",
    "        betas.append(self.b)\n",
    "        # condtion (when satisfied algorithm will stop)\n",
    "        cond = 1\n",
    "        # iteration counter\n",
    "        i = 1\n",
    "        \n",
    "        \n",
    "        while cond > stop_condition and i < max_iter:\n",
    "            \n",
    "            if method == \"GD\":\n",
    "                if self.multiclass == 1:\n",
    "                    gradient = logit.loglikelihood_multiclass_der(self.X, self.y, self.b) / X.shape[0]\n",
    "                    print(gradient.shape, self.y.shape)\n",
    "                    gradient = np.append(gradient, np.zeros(shape = (gradient.shape[0],1)), axis = 1)  \n",
    "                else:\n",
    "                    gradient = np.mean(logit.cross_entrophy_der(self.X, self.y, self.b), axis = 0).reshape(-1,1)\n",
    "                    \n",
    "                # updating weights\n",
    "                beta_old = self.b\n",
    "                self.b = beta_old - lr * gradient\n",
    "                betas.append(self.b)\n",
    "                \n",
    "                # update the value of condition\n",
    "                cond = np.max(np.abs((beta_old - self.b) / beta_old))\n",
    "                #print(cond)\n",
    "\n",
    "                # loss function after one iteration\n",
    "                p = logit.predict_proba(self, self.X)\n",
    "                if self.multiclass == 1:\n",
    "                    loss.append(logit.loglikelihood_multiclass(self.X, self.y, self.b))\n",
    "                else:\n",
    "                    loss.append(logit.cross_entrophy(self.y, p))\n",
    "                    \n",
    "            \n",
    "            elif method == \"SGD\":\n",
    "                for j in range(self.X.shape[0]):\n",
    "                    gradient = logit.cross_entrophy_der(self.X[j,:].reshape(1,-1), self.y[j,:], self.b).reshape(-1,1)\n",
    "                    beta_old = self.b\n",
    "                    self.b = beta_old - lr * gradient\n",
    "                    cond = np.max(np.abs((beta_old - self.b) / beta_old))\n",
    "                    \n",
    "                    # loss function after one calculation\n",
    "                    betas.append(self.b)       \n",
    "                    p = logit.sigmoid(self.X @ self.b)\n",
    "                    loss.append(logit.cross_entrophy(self.y, p))\n",
    "                    \n",
    "                    if cond < stop_condition:\n",
    "                        print(\"Breaking\")\n",
    "                        break\n",
    "                print(cond)\n",
    "                    \n",
    "                    \n",
    "\n",
    "            elif method == \"IRLS\":                       \n",
    "                p = logit.sigmoid(self.X @ self.b).reshape(-1,1)\n",
    "                # diagonal T x T matrix with values \"p_i * (1-p_i)\" on the diagonal\n",
    "                W = np.diag(np.diag(p @ (1 - p).T)) + 1e-6\n",
    "                # T x 1 wector of transformed response variable\n",
    "                z = self.X @ self.b + np.linalg.inv(W) @ (self.y - p)\n",
    "                \n",
    "                # updating beta \n",
    "                beta_old = self.b\n",
    "                self.b = np.linalg.inv(self.X.T @ W @ self.X) @ self.X.T @ W @ z\n",
    "                betas.append(self.b)\n",
    "                # updating condition\n",
    "                cond = np.max(np.abs((beta_old - self.b) / beta_old))\n",
    "                # loss function after one iteration\n",
    "                p = logit.predict_proba(self, self.X)\n",
    "                loss.append(logit.cross_entrophy(self.y, p))\n",
    "                \n",
    "            elif method == \"IRLS2\":\n",
    "                p = logit.sigmoid(self.X @ self.b).reshape(-1,1)\n",
    "                W = p * (1 - p) + 1e-6# vector\n",
    " \n",
    "                # T x 1 wector of transformed response variable\n",
    "                z = self.X @ self.b + (1 / W) * (self.y - p)\n",
    "                X_temp = W * self.X\n",
    "                # updating beta \n",
    "                beta_old = self.b\n",
    "                self.b = np.linalg.inv(self.X.T @ X_temp) @ X_temp.T @ z\n",
    "                betas.append(self.b)\n",
    "                # updating condition\n",
    "                cond = np.max(np.abs((beta_old - self.b) / beta_old))\n",
    "                # loss function after one iteration\n",
    "                p = logit.predict_proba(self, self.X)\n",
    "                loss.append(logit.cross_entrophy(self.y, p))\n",
    "                \n",
    "            elif method == \"IRLS3\":\n",
    "                p = logit.sigmoid(self.X @ self.b).reshape(-1,1)\n",
    "                W = p * (1 - p)# vector\n",
    "                X_temp = W * self.X\n",
    "                beta_old = self.b\n",
    "                self.b = self.b + np.linalg.inv(self.X.T @ X_temp) @ self.X.T @ (self.y - p)\n",
    "                betas.append(self.b)\n",
    "                # updating condition\n",
    "                cond = np.max(np.abs((beta_old - self.b) / beta_old))\n",
    "                # loss function after one iteration\n",
    "                p = logit.predict_proba(self, self.X)\n",
    "                loss.append(logit.cross_entrophy(self.y, p))\n",
    "                \n",
    "                \n",
    "                \n",
    "            if verbose == True:\n",
    "                print(\"iteration\", i, \"loss function value =\", loss[-1])\n",
    "                \n",
    "            i+=1\n",
    "            \n",
    "\n",
    "        #print(cond)\n",
    "        return loss, betas\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 loss function value = 681.4784088924021\n",
      "iteration 2 loss function value = 664.5674222089688\n",
      "iteration 3 loss function value = 656.0822928811056\n",
      "iteration 4 loss function value = 649.7065590743034\n",
      "iteration 5 loss function value = 644.1043382205206\n",
      "iteration 6 loss function value = 638.9722813103044\n",
      "iteration 7 loss function value = 634.2271594404315\n",
      "iteration 8 loss function value = 629.8333744126156\n",
      "iteration 9 loss function value = 625.7660257781237\n",
      "iteration 10 loss function value = 622.0030958822567\n",
      "iteration 11 loss function value = 618.5238600210357\n",
      "iteration 12 loss function value = 615.3086004071502\n",
      "iteration 13 loss function value = 612.3385777371825\n",
      "iteration 14 loss function value = 609.5960423166636\n",
      "iteration 15 loss function value = 607.0642413748172\n",
      "iteration 16 loss function value = 604.7274155088385\n",
      "iteration 17 loss function value = 602.5707845757089\n",
      "iteration 18 loss function value = 600.5805247243457\n",
      "iteration 19 loss function value = 598.743738384348\n",
      "iteration 20 loss function value = 597.0484188847222\n",
      "iteration 21 loss function value = 595.4834111769153\n",
      "iteration 22 loss function value = 594.0383699310926\n",
      "iteration 23 loss function value = 592.7037160777759\n",
      "iteration 24 loss function value = 591.4705926847507\n",
      "iteration 25 loss function value = 590.3308208944013\n",
      "iteration 26 loss function value = 589.2768565004433\n",
      "iteration 27 loss function value = 588.3017476154748\n",
      "iteration 28 loss function value = 587.3990937711174\n",
      "iteration 29 loss function value = 586.5630066996177\n",
      "iteration 30 loss function value = 585.7880729681281\n",
      "iteration 31 loss function value = 585.0693185728969\n",
      "iteration 32 loss function value = 584.4021755486215\n",
      "iteration 33 loss function value = 583.7824506066552\n",
      "iteration 34 loss function value = 583.2062957831363\n",
      "iteration 35 loss function value = 582.6701810530157\n",
      "iteration 36 loss function value = 582.1708688471974\n",
      "iteration 37 loss function value = 581.7053903964584\n",
      "iteration 38 loss function value = 581.2710238165464\n",
      "iteration 39 loss function value = 580.8652738430314\n",
      "iteration 40 loss function value = 580.48585312143\n",
      "iteration 41 loss function value = 580.1306649572238\n",
      "iteration 42 loss function value = 579.7977874311812\n",
      "iteration 43 loss function value = 579.4854587874427\n",
      "iteration 44 loss function value = 579.1920640048284\n",
      "iteration 45 loss function value = 578.9161224654799\n",
      "iteration 46 loss function value = 578.6562766390646\n",
      "iteration 47 loss function value = 578.4112817051499\n",
      "iteration 48 loss function value = 578.1799960408823\n",
      "iteration 49 loss function value = 577.9613725056613\n",
      "iteration 50 loss function value = 577.7544504590089\n",
      "iteration 51 loss function value = 577.5583484522372\n",
      "iteration 52 loss function value = 577.372257538761\n",
      "iteration 53 loss function value = 577.1954351519793\n",
      "iteration 54 loss function value = 577.0271995035007\n",
      "iteration 55 loss function value = 576.8669244581492\n",
      "iteration 56 loss function value = 576.7140348456073\n",
      "iteration 57 loss function value = 576.5680021717693\n",
      "iteration 58 loss function value = 576.428340695866\n",
      "iteration 59 loss function value = 576.2946038422066\n",
      "iteration 60 loss function value = 576.1663809179594\n",
      "iteration 61 loss function value = 576.0432941107804\n",
      "iteration 62 loss function value = 575.9249957422993\n",
      "iteration 63 loss function value = 575.8111657555016\n",
      "iteration 64 loss function value = 575.7015094159119\n",
      "iteration 65 loss function value = 575.5957552082002\n",
      "iteration 66 loss function value = 575.4936529114057\n",
      "iteration 67 loss function value = 575.3949718374176\n",
      "iteration 68 loss function value = 575.2994992186798\n",
      "iteration 69 loss function value = 575.2070387322855\n",
      "iteration 70 loss function value = 575.1174091487551\n",
      "iteration 71 loss function value = 575.0304430947851\n",
      "iteration 72 loss function value = 574.9459859201986\n",
      "iteration 73 loss function value = 574.8638946601618\n",
      "iteration 74 loss function value = 574.7840370845186\n",
      "iteration 75 loss function value = 574.7062908267866\n",
      "iteration 76 loss function value = 574.6305425860171\n",
      "iteration 77 loss function value = 574.5566873953003\n",
      "iteration 78 loss function value = 574.4846279512432\n",
      "iteration 79 loss function value = 574.4142739992294\n",
      "iteration 80 loss function value = 574.3455417697281\n",
      "iteration 81 loss function value = 574.2783534613203\n",
      "iteration 82 loss function value = 574.2126367664871\n",
      "iteration 83 loss function value = 574.1483244365462\n",
      "iteration 84 loss function value = 574.0853538824309\n",
      "iteration 85 loss function value = 574.0236668082935\n",
      "iteration 86 loss function value = 573.963208875169\n",
      "iteration 87 loss function value = 573.9039293921779\n",
      "iteration 88 loss function value = 573.8457810329559\n",
      "iteration 89 loss function value = 573.788719575201\n",
      "iteration 90 loss function value = 573.7327036614063\n",
      "iteration 91 loss function value = 573.6776945790114\n",
      "iteration 92 loss function value = 573.6236560583532\n",
      "iteration 93 loss function value = 573.5705540869412\n",
      "iteration 94 loss function value = 573.5183567386987\n",
      "iteration 95 loss function value = 573.4670340169312\n",
      "iteration 96 loss function value = 573.4165577098897\n",
      "iteration 97 loss function value = 573.366901257886\n",
      "iteration 98 loss function value = 573.3180396310087\n",
      "iteration 99 loss function value = 573.2699492165723\n",
      "iteration 100 loss function value = 573.2226077154945\n",
      "iteration 101 loss function value = 573.1759940468745\n",
      "iteration 102 loss function value = 573.1300882601003\n",
      "iteration 103 loss function value = 573.08487145387\n",
      "iteration 104 loss function value = 573.0403257015653\n",
      "iteration 105 loss function value = 572.9964339824583\n",
      "iteration 106 loss function value = 572.9531801182807\n",
      "iteration 107 loss function value = 572.9105487147201\n",
      "iteration 108 loss function value = 572.8685251074446\n",
      "iteration 109 loss function value = 572.827095312291\n",
      "iteration 110 loss function value = 572.7862459792818\n",
      "iteration 111 loss function value = 572.7459643501625\n",
      "iteration 112 loss function value = 572.7062382191776\n",
      "iteration 113 loss function value = 572.6670558968267\n",
      "iteration 114 loss function value = 572.6284061763614\n",
      "iteration 115 loss function value = 572.5902783028066\n",
      "iteration 116 loss function value = 572.5526619443044\n",
      "iteration 117 loss function value = 572.5155471655962\n",
      "iteration 118 loss function value = 572.4789244034765\n",
      "iteration 119 loss function value = 572.4427844440581\n",
      "iteration 120 loss function value = 572.4071184017121\n",
      "iteration 121 loss function value = 572.371917699546\n",
      "iteration 122 loss function value = 572.3371740513037\n",
      "iteration 123 loss function value = 572.3028794445723\n",
      "iteration 124 loss function value = 572.2690261251988\n",
      "iteration 125 loss function value = 572.2356065828183\n",
      "iteration 126 loss function value = 572.20261353741\n",
      "iteration 127 loss function value = 572.1700399268027\n",
      "iteration 128 loss function value = 572.1378788950537\n",
      "iteration 129 loss function value = 572.106123781637\n",
      "iteration 130 loss function value = 572.0747681113778\n",
      "iteration 131 loss function value = 572.043805585076\n",
      "iteration 132 loss function value = 572.0132300707678\n",
      "iteration 133 loss function value = 571.9830355955759\n",
      "iteration 134 loss function value = 571.9532163381051\n",
      "iteration 135 loss function value = 571.9237666213411\n",
      "iteration 136 loss function value = 571.8946809060154\n",
      "iteration 137 loss function value = 571.8659537844035\n",
      "iteration 138 loss function value = 571.8375799745204\n",
      "iteration 139 loss function value = 571.8095543146877\n",
      "iteration 140 loss function value = 571.7818717584438\n",
      "iteration 141 loss function value = 571.7545273697713\n",
      "iteration 142 loss function value = 571.7275163186191\n",
      "iteration 143 loss function value = 571.7008338766991\n",
      "iteration 144 loss function value = 571.6744754135359\n",
      "iteration 145 loss function value = 571.648436392752\n",
      "iteration 146 loss function value = 571.6227123685724\n",
      "iteration 147 loss function value = 571.5972989825319\n",
      "iteration 148 loss function value = 571.5721919603727\n",
      "iteration 149 loss function value = 571.5473871091162\n",
      "iteration 150 loss function value = 571.5228803143004\n",
      "iteration 151 loss function value = 571.498667537367\n",
      "iteration 152 loss function value = 571.4747448131934\n",
      "iteration 153 loss function value = 571.4511082477546\n",
      "iteration 154 loss function value = 571.4277540159094\n",
      "iteration 155 loss function value = 571.4046783593019\n",
      "iteration 156 loss function value = 571.3818775843688\n",
      "iteration 157 loss function value = 571.3593480604495\n",
      "iteration 158 loss function value = 571.3370862179875\n",
      "iteration 159 loss function value = 571.3150885468203\n",
      "iteration 160 loss function value = 571.2933515945513\n",
      "iteration 161 loss function value = 571.2718719649988\n",
      "iteration 162 loss function value = 571.250646316716\n",
      "iteration 163 loss function value = 571.2296713615784\n",
      "iteration 164 loss function value = 571.2089438634351\n",
      "iteration 165 loss function value = 571.1884606368187\n",
      "iteration 166 loss function value = 571.1682185457108\n",
      "iteration 167 loss function value = 571.1482145023605\n",
      "iteration 168 loss function value = 571.1284454661507\n",
      "iteration 169 loss function value = 571.1089084425121\n",
      "iteration 170 loss function value = 571.0896004818803\n",
      "iteration 171 loss function value = 571.0705186786927\n",
      "iteration 172 loss function value = 571.0516601704257\n",
      "iteration 173 loss function value = 571.0330221366685\n",
      "iteration 174 loss function value = 571.0146017982288\n",
      "iteration 175 loss function value = 570.9963964162746\n",
      "iteration 176 loss function value = 570.9784032915039\n",
      "iteration 177 loss function value = 570.9606197633452\n",
      "iteration 178 loss function value = 570.9430432091849\n",
      "iteration 179 loss function value = 570.9256710436204\n",
      "iteration 180 loss function value = 570.9085007177391\n",
      "iteration 181 loss function value = 570.8915297184199\n",
      "iteration 182 loss function value = 570.8747555676575\n",
      "iteration 183 loss function value = 570.8581758219077\n",
      "iteration 184 loss function value = 570.8417880714526\n",
      "iteration 185 loss function value = 570.8255899397864\n",
      "iteration 186 loss function value = 570.8095790830164\n",
      "iteration 187 loss function value = 570.7937531892852\n",
      "iteration 188 loss function value = 570.7781099782071\n",
      "iteration 189 loss function value = 570.7626472003203\n",
      "iteration 190 loss function value = 570.7473626365557\n",
      "iteration 191 loss function value = 570.7322540977189\n",
      "iteration 192 loss function value = 570.7173194239858\n",
      "iteration 193 loss function value = 570.7025564844133\n",
      "iteration 194 loss function value = 570.6879631764607\n",
      "iteration 195 loss function value = 570.6735374255236\n",
      "iteration 196 loss function value = 570.6592771844804\n",
      "iteration 197 loss function value = 570.6451804332494\n",
      "iteration 198 loss function value = 570.6312451783558\n",
      "iteration 199 loss function value = 570.6174694525107\n",
      "iteration 200 loss function value = 570.6038513141987\n",
      "iteration 201 loss function value = 570.590388847275\n",
      "iteration 202 loss function value = 570.5770801605731\n",
      "iteration 203 loss function value = 570.5639233875193\n",
      "iteration 204 loss function value = 570.550916685757\n",
      "iteration 205 loss function value = 570.5380582367795\n",
      "iteration 206 loss function value = 570.5253462455695\n",
      "iteration 207 loss function value = 570.512778940247\n",
      "iteration 208 loss function value = 570.500354571725\n",
      "iteration 209 loss function value = 570.4880714133707\n",
      "iteration 210 loss function value = 570.4759277606763\n",
      "iteration 211 loss function value = 570.4639219309336\n",
      "iteration 212 loss function value = 570.4520522629164\n",
      "iteration 213 loss function value = 570.4403171165701\n",
      "iteration 214 loss function value = 570.4287148727054\n",
      "iteration 215 loss function value = 570.4172439326994\n",
      "iteration 216 loss function value = 570.4059027182013\n",
      "iteration 217 loss function value = 570.3946896708449\n",
      "iteration 218 loss function value = 570.3836032519647\n",
      "iteration 219 loss function value = 570.3726419423197\n",
      "iteration 220 loss function value = 570.3618042418191\n",
      "iteration 221 loss function value = 570.3510886692563\n",
      "iteration 222 loss function value = 570.3404937620448\n",
      "iteration 223 loss function value = 570.3300180759604\n",
      "iteration 224 loss function value = 570.3196601848879\n",
      "iteration 225 loss function value = 570.3094186805707\n",
      "iteration 226 loss function value = 570.2992921723672\n",
      "iteration 227 loss function value = 570.2892792870091\n",
      "iteration 228 loss function value = 570.2793786683649\n",
      "iteration 229 loss function value = 570.2695889772069\n",
      "iteration 230 loss function value = 570.2599088909828\n",
      "iteration 231 loss function value = 570.2503371035903\n",
      "iteration 232 loss function value = 570.2408723251558\n",
      "iteration 233 loss function value = 570.2315132818167\n",
      "iteration 234 loss function value = 570.2222587155064\n",
      "iteration 235 loss function value = 570.2131073837447\n",
      "iteration 236 loss function value = 570.204058059429\n",
      "iteration 237 loss function value = 570.1951095306315\n",
      "iteration 238 loss function value = 570.1862606003968\n",
      "iteration 239 loss function value = 570.1775100865449\n",
      "iteration 240 loss function value = 570.1688568214763\n",
      "iteration 241 loss function value = 570.1602996519796\n",
      "iteration 242 loss function value = 570.151837439044\n",
      "iteration 243 loss function value = 570.1434690576717\n",
      "iteration 244 loss function value = 570.1351933966962\n",
      "iteration 245 loss function value = 570.1270093586006\n",
      "iteration 246 loss function value = 570.1189158593413\n",
      "iteration 247 loss function value = 570.1109118281715\n",
      "iteration 248 loss function value = 570.1029962074699\n",
      "iteration 249 loss function value = 570.0951679525703\n",
      "iteration 250 loss function value = 570.0874260315942\n",
      "iteration 251 loss function value = 570.079769425286\n",
      "iteration 252 loss function value = 570.0721971268506\n",
      "iteration 253 loss function value = 570.0647081417924\n",
      "iteration 254 loss function value = 570.0573014877582\n",
      "iteration 255 loss function value = 570.0499761943815\n",
      "iteration 256 loss function value = 570.0427313031289\n",
      "iteration 257 loss function value = 570.0355658671486\n",
      "iteration 258 loss function value = 570.0284789511218\n",
      "iteration 259 loss function value = 570.0214696311156\n",
      "iteration 260 loss function value = 570.0145369944378\n",
      "iteration 261 loss function value = 570.0076801394948\n",
      "iteration 262 loss function value = 570.0008981756496\n",
      "iteration 263 loss function value = 569.9941902230837\n",
      "iteration 264 loss function value = 569.98755541266\n",
      "iteration 265 loss function value = 569.9809928857871\n",
      "iteration 266 loss function value = 569.9745017942876\n",
      "iteration 267 loss function value = 569.9680813002648\n",
      "iteration 268 loss function value = 569.9617305759739\n",
      "iteration 269 loss function value = 569.9554488036947\n",
      "iteration 270 loss function value = 569.949235175605\n",
      "iteration 271 loss function value = 569.9430888936561\n",
      "iteration 272 loss function value = 569.93700916945\n",
      "iteration 273 loss function value = 569.9309952241192\n",
      "iteration 274 loss function value = 569.9250462882065\n",
      "iteration 275 loss function value = 569.9191616015473\n",
      "iteration 276 loss function value = 569.913340413154\n",
      "iteration 277 loss function value = 569.9075819811001\n",
      "iteration 278 loss function value = 569.9018855724084\n",
      "iteration 279 loss function value = 569.8962504629388\n",
      "iteration 280 loss function value = 569.890675937278\n",
      "iteration 281 loss function value = 569.8851612886309\n",
      "iteration 282 loss function value = 569.8797058187137\n",
      "iteration 283 loss function value = 569.8743088376473\n",
      "iteration 284 loss function value = 569.8689696638539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 285 loss function value = 569.8636876239531\n",
      "iteration 286 loss function value = 569.8584620526608\n",
      "iteration 287 loss function value = 569.8532922926881\n",
      "iteration 288 loss function value = 569.848177694643\n",
      "iteration 289 loss function value = 569.843117616932\n",
      "iteration 290 loss function value = 569.8381114256642\n",
      "iteration 291 loss function value = 569.8331584945556\n",
      "iteration 292 loss function value = 569.828258204835\n",
      "iteration 293 loss function value = 569.823409945152\n",
      "iteration 294 loss function value = 569.8186131114846\n",
      "iteration 295 loss function value = 569.8138671070488\n",
      "iteration 296 loss function value = 569.8091713422103\n",
      "iteration 297 loss function value = 569.8045252343952\n",
      "iteration 298 loss function value = 569.799928208004\n",
      "iteration 299 loss function value = 569.7953796943252\n",
      "iteration 300 loss function value = 569.7908791314508\n",
      "iteration 301 loss function value = 569.7864259641929\n",
      "iteration 302 loss function value = 569.7820196440007\n",
      "iteration 303 loss function value = 569.7776596288792\n",
      "iteration 304 loss function value = 569.7733453833087\n",
      "iteration 305 loss function value = 569.7690763781654\n",
      "iteration 306 loss function value = 569.764852090643\n",
      "iteration 307 loss function value = 569.7606720041753\n",
      "iteration 308 loss function value = 569.7565356083593\n",
      "iteration 309 loss function value = 569.7524423988805\n",
      "iteration 310 loss function value = 569.7483918774379\n",
      "iteration 311 loss function value = 569.7443835516707\n",
      "iteration 312 loss function value = 569.7404169350849\n",
      "iteration 313 loss function value = 569.7364915469832\n",
      "iteration 314 loss function value = 569.7326069123922\n",
      "iteration 315 loss function value = 569.728762561994\n",
      "iteration 316 loss function value = 569.7249580320565\n",
      "iteration 317 loss function value = 569.7211928643655\n",
      "iteration 318 loss function value = 569.7174666061571\n",
      "iteration 319 loss function value = 569.7137788100517\n",
      "iteration 320 loss function value = 569.7101290339879\n",
      "iteration 321 loss function value = 569.7065168411581\n",
      "iteration 322 loss function value = 569.702941799944\n",
      "iteration 323 loss function value = 569.6994034838543\n",
      "iteration 324 loss function value = 569.6959014714614\n",
      "iteration 325 loss function value = 569.6924353463405\n",
      "iteration 326 loss function value = 569.6890046970084\n",
      "iteration 327 loss function value = 569.6856091168632\n",
      "iteration 328 loss function value = 569.682248204126\n",
      "iteration 329 loss function value = 569.678921561781\n",
      "iteration 330 loss function value = 569.6756287975185\n",
      "iteration 331 loss function value = 569.6723695236776\n",
      "iteration 332 loss function value = 569.6691433571898\n",
      "iteration 333 loss function value = 569.6659499195234\n",
      "iteration 334 loss function value = 569.6627888366281\n",
      "iteration 335 loss function value = 569.6596597388813\n",
      "iteration 336 loss function value = 569.6565622610337\n",
      "iteration 337 loss function value = 569.6534960421568\n",
      "iteration 338 loss function value = 569.6504607255911\n",
      "iteration 339 loss function value = 569.6474559588928\n",
      "iteration 340 loss function value = 569.6444813937851\n",
      "iteration 341 loss function value = 569.641536686105\n",
      "iteration 342 loss function value = 569.6386214957565\n",
      "iteration 343 loss function value = 569.6357354866593\n",
      "iteration 344 loss function value = 569.632878326701\n",
      "iteration 345 loss function value = 569.6300496876889\n",
      "iteration 346 loss function value = 569.6272492453036\n",
      "iteration 347 loss function value = 569.6244766790505\n",
      "iteration 348 loss function value = 569.6217316722159\n",
      "iteration 349 loss function value = 569.6190139118189\n",
      "iteration 350 loss function value = 569.6163230885685\n",
      "iteration 351 loss function value = 569.6136588968175\n",
      "iteration 352 loss function value = 569.6110210345196\n",
      "iteration 353 loss function value = 569.6084092031857\n",
      "iteration 354 loss function value = 569.6058231078409\n",
      "iteration 355 loss function value = 569.6032624569822\n",
      "iteration 356 loss function value = 569.6007269625368\n",
      "iteration 357 loss function value = 569.5982163398207\n",
      "iteration 358 loss function value = 569.5957303074979\n",
      "iteration 359 loss function value = 569.5932685875406\n",
      "iteration 360 loss function value = 569.5908309051882\n",
      "iteration 361 loss function value = 569.5884169889096\n",
      "iteration 362 loss function value = 569.5860265703627\n",
      "iteration 363 loss function value = 569.5836593843578\n",
      "iteration 364 loss function value = 569.5813151688183\n",
      "iteration 365 loss function value = 569.5789936647441\n",
      "iteration 366 loss function value = 569.576694616174\n",
      "iteration 367 loss function value = 569.5744177701501\n",
      "iteration 368 loss function value = 569.5721628766812\n",
      "iteration 369 loss function value = 569.5699296887069\n",
      "iteration 370 loss function value = 569.567717962063\n",
      "iteration 371 loss function value = 569.565527455447\n",
      "iteration 372 loss function value = 569.5633579303824\n",
      "iteration 373 loss function value = 569.5612091511869\n",
      "iteration 374 loss function value = 569.5590808849367\n",
      "iteration 375 loss function value = 569.5569729014354\n",
      "iteration 376 loss function value = 569.5548849731802\n",
      "iteration 377 loss function value = 569.5528168753294\n",
      "iteration 378 loss function value = 569.5507683856721\n",
      "iteration 379 loss function value = 569.5487392845947\n",
      "iteration 380 loss function value = 569.5467293550514\n",
      "iteration 381 loss function value = 569.5447383825324\n",
      "iteration 382 loss function value = 569.5427661550344\n",
      "iteration 383 loss function value = 569.5408124630302\n",
      "iteration 384 loss function value = 569.5388770994391\n",
      "iteration 385 loss function value = 569.5369598595978\n",
      "iteration 386 loss function value = 569.5350605412314\n",
      "iteration 387 loss function value = 569.5331789444249\n",
      "iteration 388 loss function value = 569.5313148715952\n",
      "iteration 389 loss function value = 569.5294681274628\n",
      "iteration 390 loss function value = 569.527638519025\n",
      "iteration 391 loss function value = 569.5258258555282\n",
      "iteration 392 loss function value = 569.5240299484408\n",
      "iteration 393 loss function value = 569.5222506114277\n",
      "iteration 394 loss function value = 569.5204876603233\n",
      "iteration 395 loss function value = 569.5187409131055\n",
      "iteration 396 loss function value = 569.5170101898709\n",
      "iteration 397 loss function value = 569.5152953128094\n",
      "iteration 398 loss function value = 569.5135961061783\n",
      "iteration 399 loss function value = 569.5119123962788\n",
      "iteration 400 loss function value = 569.5102440114314\n",
      "iteration 401 loss function value = 569.5085907819513\n",
      "iteration 402 loss function value = 569.5069525401254\n",
      "iteration 403 loss function value = 569.5053291201885\n",
      "iteration 404 loss function value = 569.5037203583001\n",
      "iteration 405 loss function value = 569.5021260925214\n",
      "iteration 406 loss function value = 569.5005461627931\n",
      "iteration 407 loss function value = 569.4989804109125\n",
      "iteration 408 loss function value = 569.4974286805123\n",
      "iteration 409 loss function value = 569.4958908170374\n",
      "iteration 410 loss function value = 569.4943666677248\n",
      "iteration 411 loss function value = 569.4928560815817\n",
      "iteration 412 loss function value = 569.4913589093644\n",
      "iteration 413 loss function value = 569.4898750035575\n",
      "iteration 414 loss function value = 569.4884042183535\n",
      "iteration 415 loss function value = 569.486946409633\n",
      "iteration 416 loss function value = 569.4855014349436\n",
      "iteration 417 loss function value = 569.4840691534814\n",
      "iteration 418 loss function value = 569.4826494260703\n",
      "iteration 419 loss function value = 569.4812421151437\n",
      "iteration 420 loss function value = 569.4798470847247\n",
      "iteration 421 loss function value = 569.4784642004073\n",
      "iteration 422 loss function value = 569.4770933293382\n",
      "iteration 423 loss function value = 569.4757343401986\n",
      "iteration 424 loss function value = 569.4743871031851\n",
      "iteration 425 loss function value = 569.4730514899924\n",
      "iteration 426 loss function value = 569.4717273737957\n",
      "iteration 427 loss function value = 569.4704146292329\n",
      "iteration 428 loss function value = 569.4691131323873\n",
      "iteration 429 loss function value = 569.4678227607703\n",
      "iteration 430 loss function value = 569.4665433933053\n",
      "iteration 431 loss function value = 569.4652749103104\n",
      "iteration 432 loss function value = 569.4640171934815\n",
      "iteration 433 loss function value = 569.462770125877\n",
      "iteration 434 loss function value = 569.4615335919011\n",
      "iteration 435 loss function value = 569.460307477288\n",
      "iteration 436 loss function value = 569.4590916690862\n",
      "iteration 437 loss function value = 569.4578860556431\n",
      "iteration 438 loss function value = 569.4566905265896\n",
      "iteration 439 loss function value = 569.4555049728247\n",
      "iteration 440 loss function value = 569.454329286501\n",
      "iteration 441 loss function value = 569.4531633610096\n",
      "iteration 442 loss function value = 569.4520070909657\n",
      "iteration 443 loss function value = 569.4508603721938\n",
      "iteration 444 loss function value = 569.449723101714\n",
      "iteration 445 loss function value = 569.4485951777273\n",
      "iteration 446 loss function value = 569.4474764996022\n",
      "iteration 447 loss function value = 569.4463669678607\n",
      "iteration 448 loss function value = 569.4452664841649\n",
      "iteration 449 loss function value = 569.444174951303\n",
      "iteration 450 loss function value = 569.443092273177\n",
      "iteration 451 loss function value = 569.4420183547885\n",
      "iteration 452 loss function value = 569.440953102227\n",
      "iteration 453 loss function value = 569.439896422656\n",
      "iteration 454 loss function value = 569.4388482243007\n",
      "iteration 455 loss function value = 569.4378084164362\n",
      "iteration 456 loss function value = 569.4367769093737\n",
      "iteration 457 loss function value = 569.4357536144502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 458 loss function value = 569.4347384440146\n",
      "iteration 459 loss function value = 569.433731311417\n",
      "iteration 460 loss function value = 569.4327321309968\n",
      "iteration 461 loss function value = 569.4317408180706\n",
      "iteration 462 loss function value = 569.430757288921\n",
      "iteration 463 loss function value = 569.4297814607854\n",
      "iteration 464 loss function value = 569.4288132518448\n",
      "iteration 465 loss function value = 569.4278525812122\n",
      "iteration 466 loss function value = 569.4268993689225\n",
      "iteration 467 loss function value = 569.4259535359209\n",
      "iteration 468 loss function value = 569.425015004053\n",
      "iteration 469 loss function value = 569.4240836960531\n",
      "iteration 470 loss function value = 569.4231595355354\n",
      "iteration 471 loss function value = 569.4222424469822\n",
      "iteration 472 loss function value = 569.4213323557346\n",
      "iteration 473 loss function value = 569.4204291879821\n",
      "iteration 474 loss function value = 569.4195328707526\n",
      "iteration 475 loss function value = 569.418643331903\n",
      "iteration 476 loss function value = 569.4177605001091\n",
      "iteration 477 loss function value = 569.4168843048561\n",
      "iteration 478 loss function value = 569.4160146764291\n",
      "iteration 479 loss function value = 569.4151515459041\n",
      "iteration 480 loss function value = 569.4142948451382\n",
      "iteration 481 loss function value = 569.4134445067609\n",
      "iteration 482 loss function value = 569.412600464165\n",
      "iteration 483 loss function value = 569.4117626514972\n",
      "iteration 484 loss function value = 569.4109310036499\n",
      "iteration 485 loss function value = 569.4101054562525\n",
      "iteration 486 loss function value = 569.4092859456624\n",
      "iteration 487 loss function value = 569.4084724089564\n",
      "iteration 488 loss function value = 569.4076647839231\n",
      "iteration 489 loss function value = 569.4068630090537\n",
      "iteration 490 loss function value = 569.4060670235341\n",
      "iteration 491 loss function value = 569.4052767672372\n",
      "iteration 492 loss function value = 569.4044921807141\n",
      "iteration 493 loss function value = 569.4037132051872\n",
      "iteration 494 loss function value = 569.4029397825414\n",
      "iteration 495 loss function value = 569.4021718553167\n",
      "iteration 496 loss function value = 569.4014093667013\n",
      "iteration 497 loss function value = 569.4006522605226\n",
      "iteration 498 loss function value = 569.3999004812413\n",
      "iteration 499 loss function value = 569.3991539739427\n",
      "iteration 500 loss function value = 569.3984126843304\n",
      "iteration 501 loss function value = 569.3976765587186\n",
      "iteration 502 loss function value = 569.3969455440251\n",
      "iteration 503 loss function value = 569.3962195877641\n",
      "iteration 504 loss function value = 569.39549863804\n",
      "iteration 505 loss function value = 569.3947826435397\n",
      "iteration 506 loss function value = 569.3940715535257\n",
      "iteration 507 loss function value = 569.3933653178306\n",
      "iteration 508 loss function value = 569.3926638868493\n",
      "iteration 509 loss function value = 569.3919672115327\n",
      "iteration 510 loss function value = 569.3912752433819\n",
      "iteration 511 loss function value = 569.3905879344408\n",
      "iteration 512 loss function value = 569.3899052372908\n",
      "iteration 513 loss function value = 569.3892271050439\n",
      "iteration 514 loss function value = 569.3885534913367\n",
      "iteration 515 loss function value = 569.3878843503242\n",
      "iteration 516 loss function value = 569.3872196366743\n",
      "iteration 517 loss function value = 569.3865593055611\n",
      "iteration 518 loss function value = 569.3859033126598\n",
      "iteration 519 loss function value = 569.3852516141401\n",
      "iteration 520 loss function value = 569.384604166661\n",
      "iteration 521 loss function value = 569.3839609273653\n",
      "iteration 522 loss function value = 569.3833218538731\n",
      "iteration 523 loss function value = 569.3826869042778\n",
      "iteration 524 loss function value = 569.3820560371385\n",
      "iteration 525 loss function value = 569.3814292114768\n",
      "iteration 526 loss function value = 569.3808063867699\n",
      "iteration 527 loss function value = 569.3801875229462\n",
      "iteration 528 loss function value = 569.3795725803795\n",
      "iteration 529 loss function value = 569.3789615198842\n",
      "iteration 530 loss function value = 569.3783543027102\n",
      "iteration 531 loss function value = 569.3777508905375\n",
      "iteration 532 loss function value = 569.3771512454721\n",
      "iteration 533 loss function value = 569.3765553300395\n",
      "iteration 534 loss function value = 569.375963107182\n",
      "iteration 535 loss function value = 569.3753745402516\n",
      "iteration 536 loss function value = 569.3747895930074\n",
      "iteration 537 loss function value = 569.374208229609\n",
      "iteration 538 loss function value = 569.3736304146136\n",
      "iteration 539 loss function value = 569.37305611297\n",
      "iteration 540 loss function value = 569.3724852900145\n",
      "iteration 541 loss function value = 569.3719179114676\n",
      "iteration 542 loss function value = 569.3713539434274\n",
      "iteration 543 loss function value = 569.3707933523671\n",
      "iteration 544 loss function value = 569.3702361051296\n",
      "iteration 545 loss function value = 569.3696821689241\n",
      "iteration 546 loss function value = 569.3691315113211\n",
      "iteration 547 loss function value = 569.3685841002485\n",
      "iteration 548 loss function value = 569.3680399039877\n",
      "iteration 549 loss function value = 569.3674988911691\n",
      "iteration 550 loss function value = 569.3669610307688\n",
      "iteration 551 loss function value = 569.3664262921038\n",
      "iteration 552 loss function value = 569.3658946448286\n",
      "iteration 553 loss function value = 569.365366058931\n",
      "iteration 554 loss function value = 569.3648405047287\n",
      "iteration 555 loss function value = 569.3643179528651\n",
      "iteration 556 loss function value = 569.3637983743056\n",
      "iteration 557 loss function value = 569.3632817403344\n",
      "iteration 558 loss function value = 569.3627680225497\n",
      "iteration 559 loss function value = 569.3622571928619\n",
      "iteration 560 loss function value = 569.3617492234878\n",
      "iteration 561 loss function value = 569.361244086949\n",
      "iteration 562 loss function value = 569.3607417560677\n",
      "iteration 563 loss function value = 569.3602422039623\n",
      "iteration 564 loss function value = 569.359745404046\n",
      "iteration 565 loss function value = 569.3592513300216\n",
      "iteration 566 loss function value = 569.3587599558791\n",
      "iteration 567 loss function value = 569.3582712558923\n",
      "iteration 568 loss function value = 569.3577852046153\n",
      "iteration 569 loss function value = 569.3573017768795\n",
      "iteration 570 loss function value = 569.3568209477903\n",
      "iteration 571 loss function value = 569.3563426927244\n",
      "iteration 572 loss function value = 569.3558669873255\n",
      "iteration 573 loss function value = 569.3553938075029\n",
      "iteration 574 loss function value = 569.3549231294272\n",
      "iteration 575 loss function value = 569.3544549295278\n",
      "iteration 576 loss function value = 569.35398918449\n",
      "iteration 577 loss function value = 569.353525871252\n",
      "iteration 578 loss function value = 569.3530649670015\n",
      "iteration 579 loss function value = 569.352606449174\n",
      "iteration 580 loss function value = 569.3521502954487\n",
      "iteration 581 loss function value = 569.3516964837465\n",
      "iteration 582 loss function value = 569.3512449922274\n",
      "iteration 583 loss function value = 569.3507957992869\n",
      "iteration 584 loss function value = 569.350348883554\n",
      "iteration 585 loss function value = 569.3499042238884\n",
      "iteration 586 loss function value = 569.3494617993779\n",
      "iteration 587 loss function value = 569.3490215893356\n",
      "iteration 588 loss function value = 569.3485835732974\n",
      "iteration 589 loss function value = 569.3481477310199\n",
      "iteration 590 loss function value = 569.3477140424773\n",
      "iteration 591 loss function value = 569.3472824878587\n",
      "iteration 592 loss function value = 569.3468530475669\n",
      "iteration 593 loss function value = 569.3464257022144\n",
      "iteration 594 loss function value = 569.3460004326223\n",
      "iteration 595 loss function value = 569.3455772198173\n",
      "iteration 596 loss function value = 569.3451560450292\n",
      "iteration 597 loss function value = 569.3447368896891\n",
      "iteration 598 loss function value = 569.3443197354268\n",
      "iteration 599 loss function value = 569.3439045640685\n",
      "iteration 600 loss function value = 569.3434913576348\n",
      "iteration 601 loss function value = 569.3430800983383\n",
      "iteration 602 loss function value = 569.3426707685816\n",
      "iteration 603 loss function value = 569.3422633509549\n",
      "iteration 604 loss function value = 569.341857828234\n",
      "iteration 605 loss function value = 569.3414541833783\n",
      "iteration 606 loss function value = 569.3410523995286\n",
      "iteration 607 loss function value = 569.3406524600049\n",
      "iteration 608 loss function value = 569.3402543483047\n",
      "iteration 609 loss function value = 569.3398580481007\n",
      "iteration 610 loss function value = 569.3394635432393\n",
      "iteration 611 loss function value = 569.3390708177376\n",
      "iteration 612 loss function value = 569.3386798557826\n",
      "iteration 613 loss function value = 569.3382906417289\n",
      "iteration 614 loss function value = 569.3379031600962\n",
      "iteration 615 loss function value = 569.3375173955685\n",
      "iteration 616 loss function value = 569.3371333329915\n",
      "iteration 617 loss function value = 569.3367509573707\n",
      "iteration 618 loss function value = 569.3363702538702\n",
      "iteration 619 loss function value = 569.3359912078106\n",
      "iteration 620 loss function value = 569.335613804667\n",
      "iteration 621 loss function value = 569.3352380300673\n",
      "iteration 622 loss function value = 569.334863869791\n",
      "iteration 623 loss function value = 569.334491309767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 624 loss function value = 569.334120336072\n",
      "iteration 625 loss function value = 569.3337509349288\n",
      "iteration 626 loss function value = 569.3333830927049\n",
      "iteration 627 loss function value = 569.3330167959105\n",
      "iteration 628 loss function value = 569.3326520311971\n",
      "iteration 629 loss function value = 569.3322887853562\n",
      "iteration 630 loss function value = 569.331927045317\n",
      "iteration 631 loss function value = 569.3315667981456\n",
      "iteration 632 loss function value = 569.3312080310429\n",
      "iteration 633 loss function value = 569.3308507313434\n",
      "iteration 634 loss function value = 569.3304948865139\n",
      "iteration 635 loss function value = 569.3301404841511\n",
      "iteration 636 loss function value = 569.3297875119813\n",
      "iteration 637 loss function value = 569.3294359578582\n",
      "iteration 638 loss function value = 569.3290858097618\n",
      "iteration 639 loss function value = 569.3287370557966\n",
      "iteration 640 loss function value = 569.328389684191\n",
      "iteration 641 loss function value = 569.3280436832948\n",
      "iteration 642 loss function value = 569.3276990415786\n",
      "iteration 643 loss function value = 569.3273557476325\n",
      "iteration 644 loss function value = 569.3270137901641\n",
      "iteration 645 loss function value = 569.3266731579981\n",
      "iteration 646 loss function value = 569.3263338400744\n",
      "iteration 647 loss function value = 569.3259958254465\n",
      "iteration 648 loss function value = 569.3256591032809\n",
      "iteration 649 loss function value = 569.3253236628558\n",
      "iteration 650 loss function value = 569.3249894935593\n",
      "iteration 651 loss function value = 569.3246565848888\n",
      "iteration 652 loss function value = 569.3243249264489\n",
      "iteration 653 loss function value = 569.3239945079516\n",
      "iteration 654 loss function value = 569.3236653192137\n",
      "iteration 655 loss function value = 569.3233373501564\n",
      "iteration 656 loss function value = 569.3230105908041\n",
      "iteration 657 loss function value = 569.3226850312828\n",
      "iteration 658 loss function value = 569.3223606618194\n",
      "iteration 659 loss function value = 569.3220374727405\n",
      "iteration 660 loss function value = 569.3217154544711\n",
      "iteration 661 loss function value = 569.321394597534\n",
      "iteration 662 loss function value = 569.3210748925478\n",
      "iteration 663 loss function value = 569.3207563302269\n",
      "iteration 664 loss function value = 569.3204389013795\n",
      "iteration 665 loss function value = 569.3201225969071\n",
      "iteration 666 loss function value = 569.3198074078039\n",
      "iteration 667 loss function value = 569.3194933251541\n",
      "iteration 668 loss function value = 569.319180340133\n",
      "iteration 669 loss function value = 569.3188684440047\n",
      "iteration 670 loss function value = 569.318557628121\n",
      "iteration 671 loss function value = 569.3182478839215\n",
      "iteration 672 loss function value = 569.3179392029317\n",
      "iteration 673 loss function value = 569.317631576762\n",
      "iteration 674 loss function value = 569.3173249971076\n",
      "iteration 675 loss function value = 569.3170194557467\n",
      "iteration 676 loss function value = 569.3167149445402\n",
      "iteration 677 loss function value = 569.3164114554302\n",
      "iteration 678 loss function value = 569.3161089804396\n",
      "iteration 679 loss function value = 569.3158075116711\n",
      "iteration 680 loss function value = 569.3155070413059\n",
      "iteration 681 loss function value = 569.3152075616034\n",
      "iteration 682 loss function value = 569.3149090649003\n",
      "iteration 683 loss function value = 569.3146115436092\n",
      "iteration 684 loss function value = 569.3143149902182\n",
      "iteration 685 loss function value = 569.3140193972902\n",
      "iteration 686 loss function value = 569.3137247574616\n",
      "iteration 687 loss function value = 569.313431063442\n",
      "iteration 688 loss function value = 569.3131383080126\n",
      "iteration 689 loss function value = 569.312846484027\n",
      "iteration 690 loss function value = 569.3125555844082\n",
      "iteration 691 loss function value = 569.3122656021499\n",
      "iteration 692 loss function value = 569.3119765303142\n",
      "iteration 693 loss function value = 569.3116883620316\n",
      "iteration 694 loss function value = 569.3114010905008\n",
      "iteration 695 loss function value = 569.3111147089862\n",
      "iteration 696 loss function value = 569.3108292108191\n",
      "iteration 697 loss function value = 569.3105445893955\n",
      "iteration 698 loss function value = 569.3102608381764\n",
      "iteration 699 loss function value = 569.3099779506867\n",
      "iteration 700 loss function value = 569.3096959205144\n",
      "iteration 701 loss function value = 569.30941474131\n",
      "iteration 702 loss function value = 569.3091344067857\n",
      "iteration 703 loss function value = 569.3088549107151\n",
      "iteration 704 loss function value = 569.3085762469324\n",
      "iteration 705 loss function value = 569.308298409331\n",
      "iteration 706 loss function value = 569.3080213918639\n",
      "iteration 707 loss function value = 569.3077451885429\n",
      "iteration 708 loss function value = 569.3074697934371\n",
      "iteration 709 loss function value = 569.3071952006732\n",
      "iteration 710 loss function value = 569.3069214044345\n",
      "iteration 711 loss function value = 569.3066483989603\n",
      "iteration 712 loss function value = 569.3063761785453\n",
      "iteration 713 loss function value = 569.3061047375387\n",
      "iteration 714 loss function value = 569.3058340703446\n",
      "iteration 715 loss function value = 569.3055641714201\n",
      "iteration 716 loss function value = 569.3052950352755\n",
      "iteration 717 loss function value = 569.305026656474\n",
      "iteration 718 loss function value = 569.3047590296296\n",
      "iteration 719 loss function value = 569.304492149409\n",
      "iteration 720 loss function value = 569.3042260105287\n",
      "iteration 721 loss function value = 569.3039606077557\n",
      "iteration 722 loss function value = 569.303695935907\n",
      "iteration 723 loss function value = 569.303431989848\n",
      "iteration 724 loss function value = 569.3031687644935\n",
      "iteration 725 loss function value = 569.3029062548062\n",
      "iteration 726 loss function value = 569.3026444557959\n",
      "iteration 727 loss function value = 569.30238336252\n",
      "iteration 728 loss function value = 569.3021229700821\n",
      "iteration 729 loss function value = 569.3018632736321\n",
      "iteration 730 loss function value = 569.3016042683653\n",
      "iteration 731 loss function value = 569.301345949522\n",
      "iteration 732 loss function value = 569.3010883123873\n",
      "iteration 733 loss function value = 569.3008313522903\n",
      "iteration 734 loss function value = 569.3005750646034\n",
      "iteration 735 loss function value = 569.3003194447426\n",
      "iteration 736 loss function value = 569.3000644881668\n",
      "iteration 737 loss function value = 569.2998101903761\n",
      "iteration 738 loss function value = 569.2995565469135\n",
      "iteration 739 loss function value = 569.2993035533628\n",
      "iteration 740 loss function value = 569.2990512053489\n",
      "iteration 741 loss function value = 569.2987994985366\n",
      "iteration 742 loss function value = 569.2985484286314\n",
      "iteration 743 loss function value = 569.2982979913781\n",
      "iteration 744 loss function value = 569.2980481825609\n",
      "iteration 745 loss function value = 569.2977989980022\n",
      "iteration 746 loss function value = 569.2975504335635\n",
      "iteration 747 loss function value = 569.2973024851435\n",
      "iteration 748 loss function value = 569.2970551486787\n",
      "iteration 749 loss function value = 569.2968084201431\n",
      "iteration 750 loss function value = 569.2965622955469\n",
      "iteration 751 loss function value = 569.2963167709368\n",
      "iteration 752 loss function value = 569.2960718423956\n",
      "iteration 753 loss function value = 569.2958275060416\n",
      "iteration 754 loss function value = 569.2955837580281\n",
      "iteration 755 loss function value = 569.2953405945436\n",
      "iteration 756 loss function value = 569.2950980118106\n",
      "iteration 757 loss function value = 569.2948560060863\n",
      "iteration 758 loss function value = 569.294614573661\n",
      "iteration 759 loss function value = 569.2943737108585\n",
      "iteration 760 loss function value = 569.294133414036\n",
      "iteration 761 loss function value = 569.2938936795831\n",
      "iteration 762 loss function value = 569.2936545039215\n",
      "iteration 763 loss function value = 569.2934158835055\n",
      "iteration 764 loss function value = 569.2931778148204\n",
      "iteration 765 loss function value = 569.2929402943832\n",
      "iteration 766 loss function value = 569.292703318742\n",
      "iteration 767 loss function value = 569.2924668844748\n",
      "iteration 768 loss function value = 569.2922309881909\n",
      "iteration 769 loss function value = 569.2919956265291\n",
      "iteration 770 loss function value = 569.291760796158\n",
      "iteration 771 loss function value = 569.2915264937755\n",
      "iteration 772 loss function value = 569.2912927161086\n",
      "iteration 773 loss function value = 569.2910594599135\n",
      "iteration 774 loss function value = 569.2908267219742\n",
      "iteration 775 loss function value = 569.2905944991033\n",
      "iteration 776 loss function value = 569.2903627881412\n",
      "iteration 777 loss function value = 569.290131585956\n",
      "iteration 778 loss function value = 569.2899008894431\n",
      "iteration 779 loss function value = 569.2896706955244\n",
      "iteration 780 loss function value = 569.2894410011493\n",
      "iteration 781 loss function value = 569.2892118032933\n",
      "iteration 782 loss function value = 569.2889830989582\n",
      "iteration 783 loss function value = 569.2887548851713\n",
      "iteration 784 loss function value = 569.2885271589863\n",
      "iteration 785 loss function value = 569.2882999174817\n",
      "iteration 786 loss function value = 569.2880731577613\n",
      "iteration 787 loss function value = 569.2878468769538\n",
      "iteration 788 loss function value = 569.2876210722125\n",
      "iteration 789 loss function value = 569.2873957407148\n",
      "iteration 790 loss function value = 569.2871708796627\n",
      "iteration 791 loss function value = 569.2869464862814\n",
      "iteration 792 loss function value = 569.2867225578204\n",
      "iteration 793 loss function value = 569.2864990915521\n",
      "iteration 794 loss function value = 569.2862760847721\n",
      "iteration 795 loss function value = 569.2860535347988\n",
      "iteration 796 loss function value = 569.2858314389734\n",
      "iteration 797 loss function value = 569.2856097946599\n",
      "iteration 798 loss function value = 569.2853885992436\n",
      "iteration 799 loss function value = 569.2851678501322\n",
      "iteration 800 loss function value = 569.2849475447556\n",
      "iteration 801 loss function value = 569.2847276805642\n",
      "iteration 802 loss function value = 569.2845082550307\n",
      "iteration 803 loss function value = 569.2842892656481\n",
      "iteration 804 loss function value = 569.2840707099308\n",
      "iteration 805 loss function value = 569.2838525854136\n",
      "iteration 806 loss function value = 569.2836348896517\n",
      "iteration 807 loss function value = 569.2834176202207\n",
      "iteration 808 loss function value = 569.2832007747159\n",
      "iteration 809 loss function value = 569.2829843507529\n",
      "iteration 810 loss function value = 569.2827683459664\n",
      "iteration 811 loss function value = 569.282552758011\n",
      "iteration 812 loss function value = 569.2823375845601\n",
      "iteration 813 loss function value = 569.2821228233065\n",
      "iteration 814 loss function value = 569.2819084719615\n",
      "iteration 815 loss function value = 569.2816945282555\n",
      "iteration 816 loss function value = 569.2814809899364\n",
      "iteration 817 loss function value = 569.2812678547718\n",
      "iteration 818 loss function value = 569.2810551205463\n",
      "iteration 819 loss function value = 569.2808427850625\n",
      "iteration 820 loss function value = 569.2806308461414\n",
      "iteration 821 loss function value = 569.2804193016208\n",
      "iteration 822 loss function value = 569.2802081493562\n",
      "iteration 823 loss function value = 569.2799973872204\n",
      "iteration 824 loss function value = 569.279787013103\n",
      "iteration 825 loss function value = 569.2795770249105\n",
      "iteration 826 loss function value = 569.2793674205664\n",
      "iteration 827 loss function value = 569.2791581980102\n",
      "iteration 828 loss function value = 569.2789493551982\n",
      "iteration 829 loss function value = 569.2787408901024\n",
      "iteration 830 loss function value = 569.2785328007116\n",
      "iteration 831 loss function value = 569.2783250850297\n",
      "iteration 832 loss function value = 569.2781177410768\n",
      "iteration 833 loss function value = 569.2779107668883\n",
      "iteration 834 loss function value = 569.2777041605154\n",
      "iteration 835 loss function value = 569.2774979200238\n",
      "iteration 836 loss function value = 569.2772920434952\n",
      "iteration 837 loss function value = 569.2770865290255\n",
      "iteration 838 loss function value = 569.2768813747259\n",
      "iteration 839 loss function value = 569.2766765787221\n",
      "iteration 840 loss function value = 569.2764721391543\n",
      "iteration 841 loss function value = 569.2762680541769\n",
      "iteration 842 loss function value = 569.276064321959\n",
      "iteration 843 loss function value = 569.2758609406835\n",
      "iteration 844 loss function value = 569.2756579085471\n",
      "iteration 845 loss function value = 569.2754552237606\n",
      "iteration 846 loss function value = 569.2752528845483\n",
      "iteration 847 loss function value = 569.2750508891481\n",
      "iteration 848 loss function value = 569.2748492358116\n",
      "iteration 849 loss function value = 569.2746479228033\n",
      "iteration 850 loss function value = 569.2744469484009\n",
      "iteration 851 loss function value = 569.2742463108953\n",
      "iteration 852 loss function value = 569.2740460085902\n",
      "iteration 853 loss function value = 569.2738460398023\n",
      "iteration 854 loss function value = 569.2736464028607\n",
      "iteration 855 loss function value = 569.273447096107\n",
      "iteration 856 loss function value = 569.2732481178953\n",
      "iteration 857 loss function value = 569.273049466592\n",
      "iteration 858 loss function value = 569.2728511405762\n",
      "iteration 859 loss function value = 569.272653138238\n",
      "iteration 860 loss function value = 569.2724554579802\n",
      "iteration 861 loss function value = 569.272258098217\n",
      "iteration 862 loss function value = 569.2720610573749\n",
      "iteration 863 loss function value = 569.2718643338915\n",
      "iteration 864 loss function value = 569.271667926216\n",
      "iteration 865 loss function value = 569.2714718328092\n",
      "iteration 866 loss function value = 569.2712760521428\n",
      "iteration 867 loss function value = 569.2710805826998\n",
      "iteration 868 loss function value = 569.2708854229745\n",
      "iteration 869 loss function value = 569.270690571472\n",
      "iteration 870 loss function value = 569.2704960267081\n",
      "iteration 871 loss function value = 569.2703017872096\n",
      "iteration 872 loss function value = 569.2701078515138\n",
      "iteration 873 loss function value = 569.2699142181687\n",
      "iteration 874 loss function value = 569.2697208857322\n",
      "iteration 875 loss function value = 569.2695278527735\n",
      "iteration 876 loss function value = 569.2693351178713\n",
      "iteration 877 loss function value = 569.2691426796146\n",
      "iteration 878 loss function value = 569.2689505366027\n",
      "iteration 879 loss function value = 569.2687586874445\n",
      "iteration 880 loss function value = 569.2685671307593\n",
      "iteration 881 loss function value = 569.2683758651757\n",
      "iteration 882 loss function value = 569.2681848893321\n",
      "iteration 883 loss function value = 569.2679942018766\n",
      "iteration 884 loss function value = 569.2678038014668\n",
      "iteration 885 loss function value = 569.2676136867697\n",
      "iteration 886 loss function value = 569.2674238564616\n",
      "iteration 887 loss function value = 569.2672343092281\n",
      "iteration 888 loss function value = 569.2670450437637\n",
      "iteration 889 loss function value = 569.2668560587726\n",
      "iteration 890 loss function value = 569.2666673529674\n",
      "iteration 891 loss function value = 569.2664789250698\n",
      "iteration 892 loss function value = 569.2662907738103\n",
      "iteration 893 loss function value = 569.2661028979282\n",
      "iteration 894 loss function value = 569.2659152961718\n",
      "iteration 895 loss function value = 569.2657279672973\n",
      "iteration 896 loss function value = 569.2655409100697\n",
      "iteration 897 loss function value = 569.2653541232628\n",
      "iteration 898 loss function value = 569.2651676056582\n",
      "iteration 899 loss function value = 569.2649813560462\n",
      "iteration 900 loss function value = 569.2647953732251\n",
      "iteration 901 loss function value = 569.2646096560012\n",
      "iteration 902 loss function value = 569.2644242031893\n",
      "iteration 903 loss function value = 569.2642390136117\n",
      "iteration 904 loss function value = 569.2640540860989\n",
      "iteration 905 loss function value = 569.2638694194891\n",
      "iteration 906 loss function value = 569.2636850126283\n",
      "iteration 907 loss function value = 569.2635008643704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 908 loss function value = 569.2633169735766\n",
      "iteration 909 loss function value = 569.2631333391159\n",
      "iteration 910 loss function value = 569.2629499598647\n",
      "iteration 911 loss function value = 569.2627668347068\n",
      "iteration 912 loss function value = 569.2625839625335\n",
      "iteration 913 loss function value = 569.2624013422433\n",
      "iteration 914 loss function value = 569.2622189727417\n",
      "iteration 915 loss function value = 569.2620368529418\n",
      "iteration 916 loss function value = 569.2618549817637\n",
      "iteration 917 loss function value = 569.2616733581342\n",
      "iteration 918 loss function value = 569.2614919809873\n",
      "iteration 919 loss function value = 569.2613108492644\n",
      "iteration 920 loss function value = 569.261129961913\n",
      "iteration 921 loss function value = 569.2609493178875\n",
      "iteration 922 loss function value = 569.2607689161493\n",
      "iteration 923 loss function value = 569.2605887556666\n",
      "iteration 924 loss function value = 569.260408835414\n",
      "iteration 925 loss function value = 569.2602291543726\n",
      "iteration 926 loss function value = 569.26004971153\n",
      "iteration 927 loss function value = 569.2598705058806\n",
      "iteration 928 loss function value = 569.2596915364246\n",
      "iteration 929 loss function value = 569.2595128021692\n",
      "iteration 930 loss function value = 569.2593343021272\n",
      "iteration 931 loss function value = 569.2591560353183\n",
      "iteration 932 loss function value = 569.2589780007679\n",
      "iteration 933 loss function value = 569.2588001975075\n",
      "iteration 934 loss function value = 569.258622624575\n",
      "iteration 935 loss function value = 569.2584452810142\n",
      "iteration 936 loss function value = 569.2582681658748\n",
      "iteration 937 loss function value = 569.2580912782123\n",
      "iteration 938 loss function value = 569.2579146170883\n",
      "iteration 939 loss function value = 569.2577381815702\n",
      "iteration 940 loss function value = 569.2575619707308\n",
      "iteration 941 loss function value = 569.2573859836489\n",
      "iteration 942 loss function value = 569.2572102194092\n",
      "iteration 943 loss function value = 569.2570346771015\n",
      "iteration 944 loss function value = 569.2568593558217\n",
      "iteration 945 loss function value = 569.2566842546706\n",
      "iteration 946 loss function value = 569.2565093727552\n",
      "iteration 947 loss function value = 569.2563347091873\n",
      "iteration 948 loss function value = 569.2561602630844\n",
      "iteration 949 loss function value = 569.2559860335695\n",
      "iteration 950 loss function value = 569.2558120197704\n",
      "iteration 951 loss function value = 569.2556382208204\n",
      "iteration 952 loss function value = 569.2554646358583\n",
      "iteration 953 loss function value = 569.2552912640276\n",
      "iteration 954 loss function value = 569.2551181044773\n",
      "iteration 955 loss function value = 569.2549451563614\n",
      "iteration 956 loss function value = 569.2547724188385\n",
      "iteration 957 loss function value = 569.2545998910728\n",
      "iteration 958 loss function value = 569.254427572233\n",
      "iteration 959 loss function value = 569.2542554614931\n",
      "iteration 960 loss function value = 569.2540835580319\n",
      "iteration 961 loss function value = 569.2539118610325\n",
      "iteration 962 loss function value = 569.2537403696834\n",
      "iteration 963 loss function value = 569.2535690831779\n",
      "iteration 964 loss function value = 569.2533980007136\n",
      "iteration 965 loss function value = 569.253227121493\n",
      "iteration 966 loss function value = 569.2530564447231\n",
      "iteration 967 loss function value = 569.2528859696157\n",
      "iteration 968 loss function value = 569.2527156953872\n",
      "iteration 969 loss function value = 569.2525456212583\n",
      "iteration 970 loss function value = 569.2523757464542\n",
      "iteration 971 loss function value = 569.2522060702047\n",
      "iteration 972 loss function value = 569.2520365917441\n",
      "iteration 973 loss function value = 569.2518673103109\n",
      "iteration 974 loss function value = 569.2516982251478\n",
      "iteration 975 loss function value = 569.2515293355025\n",
      "iteration 976 loss function value = 569.251360640626\n",
      "iteration 977 loss function value = 569.2511921397744\n",
      "iteration 978 loss function value = 569.2510238322077\n",
      "iteration 979 loss function value = 569.2508557171898\n",
      "iteration 980 loss function value = 569.2506877939891\n",
      "iteration 981 loss function value = 569.2505200618781\n",
      "iteration 982 loss function value = 569.2503525201332\n",
      "iteration 983 loss function value = 569.2501851680352\n",
      "iteration 984 loss function value = 569.2500180048683\n",
      "iteration 985 loss function value = 569.2498510299213\n",
      "iteration 986 loss function value = 569.2496842424866\n",
      "iteration 987 loss function value = 569.2495176418608\n",
      "iteration 988 loss function value = 569.2493512273443\n",
      "iteration 989 loss function value = 569.2491849982409\n",
      "iteration 990 loss function value = 569.2490189538591\n",
      "iteration 991 loss function value = 569.2488530935104\n",
      "iteration 992 loss function value = 569.2486874165108\n",
      "iteration 993 loss function value = 569.2485219221794\n",
      "iteration 994 loss function value = 569.2483566098393\n",
      "iteration 995 loss function value = 569.2481914788175\n",
      "iteration 996 loss function value = 569.2480265284441\n",
      "iteration 997 loss function value = 569.2478617580537\n",
      "iteration 998 loss function value = 569.2476971669835\n",
      "iteration 999 loss function value = 569.247532754575\n",
      "iteration 1000 loss function value = 569.2473685201729\n",
      "iteration 1001 loss function value = 569.2472044631259\n",
      "iteration 1002 loss function value = 569.2470405827853\n",
      "iteration 1003 loss function value = 569.246876878507\n",
      "iteration 1004 loss function value = 569.2467133496493\n",
      "iteration 1005 loss function value = 569.2465499955748\n",
      "iteration 1006 loss function value = 569.246386815649\n",
      "iteration 1007 loss function value = 569.2462238092405\n",
      "iteration 1008 loss function value = 569.2460609757222\n",
      "iteration 1009 loss function value = 569.2458983144692\n",
      "iteration 1010 loss function value = 569.2457358248607\n",
      "iteration 1011 loss function value = 569.245573506279\n",
      "iteration 1012 loss function value = 569.2454113581093\n",
      "iteration 1013 loss function value = 569.2452493797405\n",
      "iteration 1014 loss function value = 569.2450875705642\n",
      "iteration 1015 loss function value = 569.2449259299756\n",
      "iteration 1016 loss function value = 569.2447644573731\n",
      "iteration 1017 loss function value = 569.2446031521578\n",
      "iteration 1018 loss function value = 569.2444420137341\n",
      "iteration 1019 loss function value = 569.2442810415098\n",
      "iteration 1020 loss function value = 569.2441202348954\n",
      "iteration 1021 loss function value = 569.2439595933045\n",
      "iteration 1022 loss function value = 569.2437991161536\n",
      "iteration 1023 loss function value = 569.2436388028625\n",
      "iteration 1024 loss function value = 569.2434786528539\n",
      "iteration 1025 loss function value = 569.2433186655533\n",
      "iteration 1026 loss function value = 569.2431588403892\n",
      "iteration 1027 loss function value = 569.2429991767932\n",
      "iteration 1028 loss function value = 569.2428396741994\n",
      "iteration 1029 loss function value = 569.2426803320452\n",
      "iteration 1030 loss function value = 569.2425211497705\n",
      "iteration 1031 loss function value = 569.2423621268183\n",
      "iteration 1032 loss function value = 569.2422032626341\n",
      "iteration 1033 loss function value = 569.2420445566667\n",
      "iteration 1034 loss function value = 569.241886008367\n",
      "iteration 1035 loss function value = 569.2417276171892\n",
      "iteration 1036 loss function value = 569.2415693825901\n",
      "iteration 1037 loss function value = 569.2414113040289\n",
      "iteration 1038 loss function value = 569.2412533809683\n",
      "iteration 1039 loss function value = 569.2410956128724\n",
      "iteration 1040 loss function value = 569.2409379992091\n",
      "iteration 1041 loss function value = 569.2407805394487\n",
      "iteration 1042 loss function value = 569.2406232330635\n",
      "iteration 1043 loss function value = 569.2404660795291\n",
      "iteration 1044 loss function value = 569.2403090783234\n",
      "iteration 1045 loss function value = 569.2401522289272\n",
      "iteration 1046 loss function value = 569.239995530823\n",
      "iteration 1047 loss function value = 569.2398389834967\n",
      "iteration 1048 loss function value = 569.2396825864364\n",
      "iteration 1049 loss function value = 569.2395263391327\n",
      "iteration 1050 loss function value = 569.2393702410786\n",
      "iteration 1051 loss function value = 569.2392142917698\n",
      "iteration 1052 loss function value = 569.2390584907042\n",
      "iteration 1053 loss function value = 569.2389028373823\n",
      "iteration 1054 loss function value = 569.238747331307\n",
      "iteration 1055 loss function value = 569.2385919719833\n",
      "iteration 1056 loss function value = 569.2384367589191\n",
      "iteration 1057 loss function value = 569.2382816916241\n",
      "iteration 1058 loss function value = 569.2381267696111\n",
      "iteration 1059 loss function value = 569.2379719923945\n",
      "iteration 1060 loss function value = 569.2378173594911\n",
      "iteration 1061 loss function value = 569.2376628704208\n",
      "iteration 1062 loss function value = 569.2375085247047\n",
      "iteration 1063 loss function value = 569.2373543218669\n",
      "iteration 1064 loss function value = 569.2372002614336\n",
      "iteration 1065 loss function value = 569.2370463429329\n",
      "iteration 1066 loss function value = 569.2368925658959\n",
      "iteration 1067 loss function value = 569.2367389298549\n",
      "iteration 1068 loss function value = 569.2365854343453\n",
      "iteration 1069 loss function value = 569.2364320789043\n",
      "iteration 1070 loss function value = 569.2362788630712\n",
      "iteration 1071 loss function value = 569.2361257863876\n",
      "iteration 1072 loss function value = 569.2359728483973\n",
      "iteration 1073 loss function value = 569.2358200486458\n",
      "iteration 1074 loss function value = 569.2356673866816\n",
      "iteration 1075 loss function value = 569.2355148620543\n",
      "iteration 1076 loss function value = 569.2353624743163\n",
      "iteration 1077 loss function value = 569.2352102230216\n",
      "iteration 1078 loss function value = 569.2350581077269\n",
      "iteration 1079 loss function value = 569.2349061279904\n",
      "iteration 1080 loss function value = 569.2347542833722\n",
      "iteration 1081 loss function value = 569.234602573435\n",
      "iteration 1082 loss function value = 569.2344509977434\n",
      "iteration 1083 loss function value = 569.2342995558633\n",
      "iteration 1084 loss function value = 569.2341482473637\n",
      "iteration 1085 loss function value = 569.2339970718145\n",
      "iteration 1086 loss function value = 569.2338460287885\n",
      "iteration 1087 loss function value = 569.2336951178597\n",
      "iteration 1088 loss function value = 569.2335443386045\n",
      "iteration 1089 loss function value = 569.233393690601\n",
      "iteration 1090 loss function value = 569.2332431734294\n",
      "iteration 1091 loss function value = 569.2330927866716\n",
      "iteration 1092 loss function value = 569.2329425299116\n",
      "iteration 1093 loss function value = 569.2327924027352\n",
      "iteration 1094 loss function value = 569.2326424047297\n",
      "iteration 1095 loss function value = 569.232492535485\n",
      "iteration 1096 loss function value = 569.2323427945923\n",
      "iteration 1097 loss function value = 569.2321931816448\n",
      "iteration 1098 loss function value = 569.2320436962374\n",
      "iteration 1099 loss function value = 569.2318943379671\n",
      "iteration 1100 loss function value = 569.2317451064324\n",
      "iteration 1101 loss function value = 569.2315960012337\n",
      "iteration 1102 loss function value = 569.2314470219733\n",
      "iteration 1103 loss function value = 569.2312981682551\n",
      "iteration 1104 loss function value = 569.2311494396847\n",
      "iteration 1105 loss function value = 569.2310008358697\n",
      "iteration 1106 loss function value = 569.2308523564193\n",
      "iteration 1107 loss function value = 569.2307040009446\n",
      "iteration 1108 loss function value = 569.2305557690579\n",
      "iteration 1109 loss function value = 569.2304076603739\n",
      "iteration 1110 loss function value = 569.2302596745083\n",
      "iteration 1111 loss function value = 569.2301118110793\n",
      "iteration 1112 loss function value = 569.2299640697058\n",
      "iteration 1113 loss function value = 569.2298164500094\n",
      "iteration 1114 loss function value = 569.2296689516124\n",
      "iteration 1115 loss function value = 569.2295215741395\n",
      "iteration 1116 loss function value = 569.2293743172168\n",
      "iteration 1117 loss function value = 569.2292271804715\n",
      "iteration 1118 loss function value = 569.2290801635335\n",
      "iteration 1119 loss function value = 569.2289332660333\n",
      "iteration 1120 loss function value = 569.2287864876034\n",
      "iteration 1121 loss function value = 569.2286398278782\n",
      "iteration 1122 loss function value = 569.2284932864931\n",
      "iteration 1123 loss function value = 569.2283468630853\n",
      "iteration 1124 loss function value = 569.2282005572938\n",
      "iteration 1125 loss function value = 569.2280543687589\n",
      "iteration 1126 loss function value = 569.2279082971224\n",
      "iteration 1127 loss function value = 569.227762342028\n",
      "iteration 1128 loss function value = 569.2276165031205\n",
      "iteration 1129 loss function value = 569.2274707800464\n",
      "iteration 1130 loss function value = 569.2273251724538\n",
      "iteration 1131 loss function value = 569.2271796799921\n",
      "iteration 1132 loss function value = 569.2270343023125\n",
      "iteration 1133 loss function value = 569.2268890390674\n",
      "iteration 1134 loss function value = 569.2267438899107\n",
      "iteration 1135 loss function value = 569.226598854498\n",
      "iteration 1136 loss function value = 569.226453932486\n",
      "iteration 1137 loss function value = 569.2263091235334\n",
      "iteration 1138 loss function value = 569.2261644272996\n",
      "iteration 1139 loss function value = 569.2260198434462\n",
      "iteration 1140 loss function value = 569.2258753716358\n",
      "iteration 1141 loss function value = 569.2257310115324\n",
      "iteration 1142 loss function value = 569.2255867628016\n",
      "iteration 1143 loss function value = 569.2254426251104\n",
      "iteration 1144 loss function value = 569.2252985981268\n",
      "iteration 1145 loss function value = 569.225154681521\n",
      "iteration 1146 loss function value = 569.2250108749637\n",
      "iteration 1147 loss function value = 569.2248671781275\n",
      "iteration 1148 loss function value = 569.2247235906864\n",
      "iteration 1149 loss function value = 569.2245801123153\n",
      "iteration 1150 loss function value = 569.2244367426908\n",
      "iteration 1151 loss function value = 569.2242934814913\n",
      "iteration 1152 loss function value = 569.2241503283955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1153 loss function value = 569.2240072830841\n",
      "iteration 1154 loss function value = 569.2238643452391\n",
      "iteration 1155 loss function value = 569.2237215145439\n",
      "iteration 1156 loss function value = 569.2235787906825\n",
      "iteration 1157 loss function value = 569.2234361733413\n",
      "iteration 1158 loss function value = 569.223293662207\n",
      "iteration 1159 loss function value = 569.2231512569683\n",
      "iteration 1160 loss function value = 569.2230089573147\n",
      "iteration 1161 loss function value = 569.2228667629375\n",
      "iteration 1162 loss function value = 569.2227246735287\n",
      "iteration 1163 loss function value = 569.2225826887818\n",
      "iteration 1164 loss function value = 569.2224408083918\n",
      "iteration 1165 loss function value = 569.2222990320545\n",
      "iteration 1166 loss function value = 569.2221573594671\n",
      "iteration 1167 loss function value = 569.2220157903284\n",
      "iteration 1168 loss function value = 569.221874324338\n",
      "iteration 1169 loss function value = 569.2217329611968\n",
      "iteration 1170 loss function value = 569.2215917006071\n",
      "iteration 1171 loss function value = 569.2214505422722\n",
      "iteration 1172 loss function value = 569.2213094858967\n",
      "iteration 1173 loss function value = 569.2211685311863\n",
      "iteration 1174 loss function value = 569.2210276778483\n",
      "iteration 1175 loss function value = 569.2208869255904\n",
      "iteration 1176 loss function value = 569.2207462741223\n",
      "iteration 1177 loss function value = 569.2206057231546\n",
      "iteration 1178 loss function value = 569.2204652723988\n",
      "iteration 1179 loss function value = 569.2203249215677\n",
      "iteration 1180 loss function value = 569.2201846703756\n",
      "iteration 1181 loss function value = 569.2200445185374\n",
      "iteration 1182 loss function value = 569.2199044657693\n",
      "iteration 1183 loss function value = 569.2197645117892\n",
      "iteration 1184 loss function value = 569.2196246563155\n",
      "iteration 1185 loss function value = 569.2194848990678\n",
      "iteration 1186 loss function value = 569.2193452397669\n",
      "iteration 1187 loss function value = 569.219205678135\n",
      "iteration 1188 loss function value = 569.2190662138951\n",
      "iteration 1189 loss function value = 569.218926846771\n",
      "iteration 1190 loss function value = 569.2187875764885\n",
      "iteration 1191 loss function value = 569.2186484027736\n",
      "iteration 1192 loss function value = 569.218509325354\n",
      "iteration 1193 loss function value = 569.218370343958\n",
      "iteration 1194 loss function value = 569.2182314583155\n",
      "iteration 1195 loss function value = 569.2180926681569\n",
      "iteration 1196 loss function value = 569.2179539732142\n",
      "iteration 1197 loss function value = 569.21781537322\n",
      "iteration 1198 loss function value = 569.2176768679082\n",
      "iteration 1199 loss function value = 569.2175384570139\n",
      "iteration 1200 loss function value = 569.217400140273\n",
      "iteration 1201 loss function value = 569.2172619174225\n",
      "iteration 1202 loss function value = 569.2171237882005\n",
      "iteration 1203 loss function value = 569.2169857523461\n",
      "iteration 1204 loss function value = 569.2168478095994\n",
      "iteration 1205 loss function value = 569.2167099597013\n",
      "iteration 1206 loss function value = 569.2165722023942\n",
      "iteration 1207 loss function value = 569.2164345374214\n",
      "iteration 1208 loss function value = 569.2162969645267\n",
      "iteration 1209 loss function value = 569.2161594834555\n",
      "iteration 1210 loss function value = 569.2160220939539\n",
      "iteration 1211 loss function value = 569.2158847957692\n",
      "iteration 1212 loss function value = 569.2157475886494\n",
      "iteration 1213 loss function value = 569.2156104723435\n",
      "iteration 1214 loss function value = 569.215473446602\n",
      "iteration 1215 loss function value = 569.2153365111758\n",
      "iteration 1216 loss function value = 569.2151996658168\n",
      "iteration 1217 loss function value = 569.2150629102781\n",
      "iteration 1218 loss function value = 569.2149262443138\n",
      "iteration 1219 loss function value = 569.2147896676788\n",
      "iteration 1220 loss function value = 569.214653180129\n",
      "iteration 1221 loss function value = 569.214516781421\n",
      "iteration 1222 loss function value = 569.2143804713128\n",
      "iteration 1223 loss function value = 569.2142442495631\n",
      "iteration 1224 loss function value = 569.2141081159316\n",
      "iteration 1225 loss function value = 569.2139720701787\n",
      "iteration 1226 loss function value = 569.213836112066\n",
      "iteration 1227 loss function value = 569.2137002413558\n",
      "iteration 1228 loss function value = 569.2135644578117\n",
      "iteration 1229 loss function value = 569.2134287611976\n",
      "iteration 1230 loss function value = 569.213293151279\n",
      "iteration 1231 loss function value = 569.2131576278217\n",
      "iteration 1232 loss function value = 569.2130221905927\n",
      "iteration 1233 loss function value = 569.2128868393601\n",
      "iteration 1234 loss function value = 569.2127515738922\n",
      "iteration 1235 loss function value = 569.212616393959\n",
      "iteration 1236 loss function value = 569.2124812993308\n",
      "iteration 1237 loss function value = 569.212346289779\n",
      "iteration 1238 loss function value = 569.212211365076\n",
      "iteration 1239 loss function value = 569.2120765249948\n",
      "iteration 1240 loss function value = 569.2119417693095\n",
      "iteration 1241 loss function value = 569.2118070977949\n",
      "iteration 1242 loss function value = 569.2116725102267\n",
      "iteration 1243 loss function value = 569.2115380063815\n",
      "iteration 1244 loss function value = 569.2114035860366\n",
      "iteration 1245 loss function value = 569.2112692489707\n",
      "iteration 1246 loss function value = 569.2111349949624\n",
      "iteration 1247 loss function value = 569.2110008237919\n",
      "iteration 1248 loss function value = 569.21086673524\n",
      "iteration 1249 loss function value = 569.2107327290881\n",
      "iteration 1250 loss function value = 569.210598805119\n",
      "iteration 1251 loss function value = 569.2104649631158\n",
      "iteration 1252 loss function value = 569.2103312028626\n",
      "iteration 1253 loss function value = 569.2101975241442\n",
      "iteration 1254 loss function value = 569.2100639267467\n",
      "iteration 1255 loss function value = 569.2099304104561\n",
      "iteration 1256 loss function value = 569.20979697506\n",
      "iteration 1257 loss function value = 569.2096636203466\n",
      "iteration 1258 loss function value = 569.2095303461049\n",
      "iteration 1259 loss function value = 569.2093971521244\n",
      "iteration 1260 loss function value = 569.2092640381957\n",
      "iteration 1261 loss function value = 569.2091310041103\n",
      "iteration 1262 loss function value = 569.2089980496601\n",
      "iteration 1263 loss function value = 569.208865174638\n",
      "iteration 1264 loss function value = 569.2087323788378\n",
      "iteration 1265 loss function value = 569.2085996620539\n",
      "iteration 1266 loss function value = 569.2084670240813\n",
      "iteration 1267 loss function value = 569.2083344647161\n",
      "iteration 1268 loss function value = 569.2082019837551\n",
      "iteration 1269 loss function value = 569.2080695809957\n",
      "iteration 1270 loss function value = 569.2079372562364\n",
      "iteration 1271 loss function value = 569.2078050092758\n",
      "iteration 1272 loss function value = 569.207672839914\n",
      "iteration 1273 loss function value = 569.2075407479514\n",
      "iteration 1274 loss function value = 569.2074087331891\n",
      "iteration 1275 loss function value = 569.2072767954293\n",
      "iteration 1276 loss function value = 569.2071449344746\n",
      "iteration 1277 loss function value = 569.2070131501287\n",
      "iteration 1278 loss function value = 569.2068814421955\n",
      "iteration 1279 loss function value = 569.20674981048\n",
      "iteration 1280 loss function value = 569.2066182547879\n",
      "iteration 1281 loss function value = 569.2064867749257\n",
      "iteration 1282 loss function value = 569.2063553707003\n",
      "iteration 1283 loss function value = 569.2062240419195\n",
      "iteration 1284 loss function value = 569.206092788392\n",
      "iteration 1285 loss function value = 569.2059616099268\n",
      "iteration 1286 loss function value = 569.2058305063342\n",
      "iteration 1287 loss function value = 569.2056994774243\n",
      "iteration 1288 loss function value = 569.2055685230089\n",
      "iteration 1289 loss function value = 569.2054376428998\n",
      "iteration 1290 loss function value = 569.20530683691\n",
      "iteration 1291 loss function value = 569.2051761048526\n",
      "iteration 1292 loss function value = 569.2050454465419\n",
      "iteration 1293 loss function value = 569.2049148617928\n",
      "iteration 1294 loss function value = 569.2047843504204\n",
      "iteration 1295 loss function value = 569.2046539122412\n",
      "iteration 1296 loss function value = 569.2045235470721\n",
      "iteration 1297 loss function value = 569.2043932547303\n",
      "iteration 1298 loss function value = 569.2042630350343\n",
      "iteration 1299 loss function value = 569.2041328878026\n",
      "iteration 1300 loss function value = 569.2040028128552\n",
      "iteration 1301 loss function value = 569.203872810012\n",
      "iteration 1302 loss function value = 569.2037428790936\n",
      "iteration 1303 loss function value = 569.2036130199219\n",
      "iteration 1304 loss function value = 569.2034832323191\n",
      "iteration 1305 loss function value = 569.2033535161077\n",
      "iteration 1306 loss function value = 569.2032238711113\n",
      "iteration 1307 loss function value = 569.2030942971539\n",
      "iteration 1308 loss function value = 569.2029647940606\n",
      "iteration 1309 loss function value = 569.2028353616563\n",
      "iteration 1310 loss function value = 569.2027059997674\n",
      "iteration 1311 loss function value = 569.2025767082205\n",
      "iteration 1312 loss function value = 569.2024474868426\n",
      "iteration 1313 loss function value = 569.2023183354621\n",
      "iteration 1314 loss function value = 569.2021892539074\n",
      "iteration 1315 loss function value = 569.2020602420073\n",
      "iteration 1316 loss function value = 569.2019312995922\n",
      "iteration 1317 loss function value = 569.201802426492\n",
      "iteration 1318 loss function value = 569.2016736225381\n",
      "iteration 1319 loss function value = 569.201544887562\n",
      "iteration 1320 loss function value = 569.2014162213961\n",
      "iteration 1321 loss function value = 569.2012876238732\n",
      "iteration 1322 loss function value = 569.2011590948266\n",
      "iteration 1323 loss function value = 569.2010306340908\n",
      "iteration 1324 loss function value = 569.2009022415\n",
      "iteration 1325 loss function value = 569.2007739168901\n",
      "iteration 1326 loss function value = 569.2006456600964\n",
      "iteration 1327 loss function value = 569.2005174709557\n",
      "iteration 1328 loss function value = 569.2003893493052\n",
      "iteration 1329 loss function value = 569.2002612949823\n",
      "iteration 1330 loss function value = 569.2001333078254\n",
      "iteration 1331 loss function value = 569.2000053876732\n",
      "iteration 1332 loss function value = 569.1998775343652\n",
      "iteration 1333 loss function value = 569.1997497477415\n",
      "iteration 1334 loss function value = 569.1996220276427\n",
      "iteration 1335 loss function value = 569.1994943739098\n",
      "iteration 1336 loss function value = 569.1993667863846\n",
      "iteration 1337 loss function value = 569.1992392649097\n",
      "iteration 1338 loss function value = 569.1991118093276\n",
      "iteration 1339 loss function value = 569.1989844194818\n",
      "iteration 1340 loss function value = 569.1988570952165\n",
      "iteration 1341 loss function value = 569.1987298363761\n",
      "iteration 1342 loss function value = 569.1986026428057\n",
      "iteration 1343 loss function value = 569.198475514351\n",
      "iteration 1344 loss function value = 569.1983484508585\n",
      "iteration 1345 loss function value = 569.1982214521747\n",
      "iteration 1346 loss function value = 569.1980945181472\n",
      "iteration 1347 loss function value = 569.1979676486236\n",
      "iteration 1348 loss function value = 569.1978408434527\n",
      "iteration 1349 loss function value = 569.1977141024831\n",
      "iteration 1350 loss function value = 569.1975874255646\n",
      "iteration 1351 loss function value = 569.1974608125471\n",
      "iteration 1352 loss function value = 569.1973342632814\n",
      "iteration 1353 loss function value = 569.1972077776186\n",
      "iteration 1354 loss function value = 569.1970813554103\n",
      "iteration 1355 loss function value = 569.1969549965086\n",
      "iteration 1356 loss function value = 569.1968287007666\n",
      "iteration 1357 loss function value = 569.1967024680372\n",
      "iteration 1358 loss function value = 569.1965762981743\n",
      "iteration 1359 loss function value = 569.1964501910322\n",
      "iteration 1360 loss function value = 569.1963241464658\n",
      "iteration 1361 loss function value = 569.1961981643306\n",
      "iteration 1362 loss function value = 569.1960722444821\n",
      "iteration 1363 loss function value = 569.195946386777\n",
      "iteration 1364 loss function value = 569.1958205910721\n",
      "iteration 1365 loss function value = 569.1956948572248\n",
      "iteration 1366 loss function value = 569.1955691850931\n",
      "iteration 1367 loss function value = 569.1954435745351\n",
      "iteration 1368 loss function value = 569.1953180254101\n",
      "iteration 1369 loss function value = 569.1951925375773\n",
      "iteration 1370 loss function value = 569.1950671108968\n",
      "iteration 1371 loss function value = 569.194941745229\n",
      "iteration 1372 loss function value = 569.1948164404348\n",
      "iteration 1373 loss function value = 569.1946911963756\n",
      "iteration 1374 loss function value = 569.1945660129134\n",
      "iteration 1375 loss function value = 569.1944408899104\n",
      "iteration 1376 loss function value = 569.1943158272297\n",
      "iteration 1377 loss function value = 569.1941908247346\n",
      "iteration 1378 loss function value = 569.1940658822889\n",
      "iteration 1379 loss function value = 569.193940999757\n",
      "iteration 1380 loss function value = 569.193816177004\n",
      "iteration 1381 loss function value = 569.1936914138948\n",
      "iteration 1382 loss function value = 569.1935667102955\n",
      "iteration 1383 loss function value = 569.1934420660721\n",
      "iteration 1384 loss function value = 569.1933174810915\n",
      "iteration 1385 loss function value = 569.193192955221\n",
      "iteration 1386 loss function value = 569.193068488328\n",
      "iteration 1387 loss function value = 569.1929440802809\n",
      "iteration 1388 loss function value = 569.1928197309483\n",
      "iteration 1389 loss function value = 569.192695440199\n",
      "iteration 1390 loss function value = 569.1925712079029\n",
      "iteration 1391 loss function value = 569.19244703393\n",
      "iteration 1392 loss function value = 569.1923229181504\n",
      "iteration 1393 loss function value = 569.1921988604353\n",
      "iteration 1394 loss function value = 569.192074860656\n",
      "iteration 1395 loss function value = 569.1919509186845\n",
      "iteration 1396 loss function value = 569.1918270343928\n",
      "iteration 1397 loss function value = 569.1917032076537\n",
      "iteration 1398 loss function value = 569.1915794383407\n",
      "iteration 1399 loss function value = 569.191455726327\n",
      "GD accuracy: 0.6622807017543859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD recall: 0.5147058823529411\n",
      "GD precision: 0.65625\n",
      "GD F measure: 0.576923076923077\n"
     ]
    }
   ],
   "source": [
    "#np.seterr(divide='ignore', invalid='ignore')\n",
    "model1 = logit(X2_train, y2_train ,add_intercept = True)\n",
    "loss1, betas1 = model1.fit(method = \"GD\", verbose = True, max_iter = 140, lr = 0.05, stop_condition = 0.00001 )\n",
    "print(\"GD accuracy:\", model1.accuracy(model1.X, model1.y))\n",
    "print(\"GD recall:\", model1.recall(model1.X, model1.y))\n",
    "print(\"GD precision:\", model1.precision(model1.X, model1.y))\n",
    "print(\"GD F measure:\", model1.f_measure(model1.X, model1.y))\n",
    "\n",
    "#print(betas1[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10444179742079095\n",
      "iteration 1 loss function value = 574.2109191754959\n",
      "0.07019864991772266\n",
      "iteration 2 loss function value = 570.8977553795115\n",
      "0.09024294053535654\n",
      "iteration 3 loss function value = 569.903888236343\n",
      "0.11109074819658281\n",
      "iteration 4 loss function value = 569.5263116012661\n",
      "0.13245049560553138\n",
      "iteration 5 loss function value = 569.3643294069557\n",
      "0.1536601975818458\n",
      "iteration 6 loss function value = 569.2861388827016\n",
      "0.17391918769301545\n",
      "iteration 7 loss function value = 569.2437228662639\n",
      "0.19240874983075965\n",
      "iteration 8 loss function value = 569.2181516563334\n",
      "0.20839017843983743\n",
      "iteration 9 loss function value = 569.2013140108285\n",
      "0.22129732768224963\n",
      "iteration 10 loss function value = 569.1894029357015\n",
      "0.23080671190125354\n",
      "iteration 11 loss function value = 569.1804598791113\n",
      "0.23686476485201813\n",
      "iteration 12 loss function value = 569.1733916360702\n",
      "0.23966560286135563\n",
      "iteration 13 loss function value = 569.167547672831\n",
      "0.2395901037656587\n",
      "iteration 14 loss function value = 569.162522777588\n",
      "0.23712755808561242\n",
      "iteration 15 loss function value = 569.1580566640266\n",
      "0.23280098060978033\n",
      "iteration 16 loss function value = 569.1539787382537\n",
      "0.2271096743466485\n",
      "iteration 17 loss function value = 569.1501757270156\n",
      "0.22049353039348732\n",
      "iteration 18 loss function value = 569.1465717955066\n",
      "0.2133168650892675\n",
      "iteration 19 loss function value = 569.1431159262941\n",
      "0.20586648301742083\n",
      "iteration 20 loss function value = 569.1397737206128\n",
      "0.19835832058797812\n",
      "iteration 21 loss function value = 569.1365219842539\n",
      "0.19094813260290863\n",
      "iteration 22 loss function value = 569.1333451089286\n",
      "0.1837431763810457\n",
      "iteration 23 loss function value = 569.1302326316709\n",
      "0.17681315686402854\n",
      "iteration 24 loss function value = 569.127177577529\n",
      "0.1701996329337838\n",
      "iteration 25 loss function value = 569.1241753286131\n",
      "0.16392366501231867\n",
      "iteration 26 loss function value = 569.1212228499082\n",
      "0.1579917957588011\n",
      "iteration 27 loss function value = 569.1183181586046\n",
      "0.1524005934018487\n",
      "iteration 28 loss function value = 569.1154599605779\n",
      "0.1471400244948019\n",
      "iteration 29 loss function value = 569.1126474020875\n",
      "0.14219590895142467\n",
      "iteration 30 loss function value = 569.1098799011106\n",
      "0.1375516747586311\n",
      "iteration 31 loss function value = 569.1071570337933\n",
      "0.13318958894382082\n",
      "iteration 32 loss function value = 569.1044784590318\n",
      "0.1290916030463798\n",
      "iteration 33 loss function value = 569.1018438693741\n",
      "0.12523991863179013\n",
      "iteration 34 loss function value = 569.0992529600087\n",
      "0.12161735195919876\n",
      "iteration 35 loss function value = 569.0967054100972\n",
      "0.11820755629446383\n",
      "iteration 36 loss function value = 569.0942008724404\n",
      "0.11499514464924734\n",
      "iteration 37 loss function value = 569.0917389686842\n",
      "0.11196574395251611\n",
      "iteration 38 loss function value = 569.0893192881297\n",
      "0.10910600294390639\n",
      "iteration 39 loss function value = 569.086941388807\n",
      "0.10640356968416534\n",
      "iteration 40 loss function value = 569.0846047998975\n",
      "0.10384704992123157\n",
      "iteration 41 loss function value = 569.0823090248867\n",
      "0.10142595417992302\n",
      "iteration 42 loss function value = 569.0800535450298\n",
      "0.0991306390165542\n",
      "iteration 43 loss function value = 569.0778378228712\n",
      "0.09695224614208606\n",
      "iteration 44 loss function value = 569.0756613056398\n",
      "0.09488264187991041\n",
      "iteration 45 loss function value = 569.07352342843\n",
      "0.0929143585487979\n",
      "iteration 46 loss function value = 569.0714236171095\n",
      "0.09104053874679457\n",
      "iteration 47 loss function value = 569.0693612909358\n",
      "0.08925488308454957\n",
      "iteration 48 loss function value = 569.0673358648771\n",
      "0.08755160162339412\n",
      "iteration 49 loss function value = 569.0653467516506\n",
      "0.08592536907595637\n",
      "iteration 50 loss function value = 569.0633933634957\n",
      "0.08437128369737729\n",
      "iteration 51 loss function value = 569.0614751137024\n",
      "0.0828848297132903\n",
      "iteration 52 loss function value = 569.0595914179219\n",
      "0.08146184308219279\n",
      "iteration 53 loss function value = 569.0577416952792\n",
      "0.080098480364437\n",
      "iteration 54 loss function value = 569.0559253693136\n",
      "0.07879119046034011\n",
      "iteration 55 loss function value = 569.0541418687646\n",
      "0.07753668898068673\n",
      "iteration 56 loss function value = 569.0523906282272\n",
      "0.07633193502042038\n",
      "iteration 57 loss function value = 569.0506710886884\n",
      "0.07517411011800253\n",
      "iteration 58 loss function value = 569.0489826979668\n",
      "0.07406059919693803\n",
      "iteration 59 loss function value = 569.0473249110628\n",
      "0.07298897330108337\n",
      "iteration 60 loss function value = 569.045697190437\n",
      "0.07195697395071225\n",
      "iteration 61 loss function value = 569.0440990062236\n",
      "0.07096249896139091\n",
      "iteration 62 loss function value = 569.0425298363901\n",
      "0.07000358958211853\n",
      "iteration 63 loss function value = 569.0409891668514\n",
      "0.06907841882276514\n",
      "iteration 64 loss function value = 569.0394764915429\n",
      "0.0681852808534172\n",
      "iteration 65 loss function value = 569.0379913124625\n",
      "0.0673225813698503\n",
      "iteration 66 loss function value = 569.0365331396829\n",
      "0.06648882882993276\n",
      "iteration 67 loss function value = 569.035101491341\n",
      "0.06568262647539405\n",
      "iteration 68 loss function value = 569.0336958936075\n",
      "0.0649026650620968\n",
      "iteration 69 loss function value = 569.0323158806397\n",
      "0.06414771622982123\n",
      "iteration 70 loss function value = 569.0309609945208\n",
      "0.06341662644962634\n",
      "iteration 71 loss function value = 569.0296307851872\n",
      "0.06270831149321344\n",
      "iteration 72 loss function value = 569.0283248103477\n",
      "0.062021751374399026\n",
      "iteration 73 loss function value = 569.0270426353939\n",
      "0.06135598571791345\n",
      "iteration 74 loss function value = 569.0257838333044\n",
      "0.060710109515293174\n",
      "iteration 75 loss function value = 569.024547984546\n",
      "0.06030335451504006\n",
      "iteration 76 loss function value = 569.023334676968\n",
      "0.060924784680737386\n",
      "iteration 77 loss function value = 569.0221435056973\n",
      "0.06155341490492054\n",
      "iteration 78 loss function value = 569.0209740730272\n",
      "0.062189347151013666\n",
      "iteration 79 loss function value = 569.0198259883088\n",
      "0.06283268521403798\n",
      "iteration 80 loss function value = 569.0186988678379\n",
      "0.063483534761759\n",
      "iteration 81 loss function value = 569.0175923347448\n",
      "0.06414200337696856\n",
      "iteration 82 loss function value = 569.0165060188804\n",
      "0.0648082006009291\n",
      "iteration 83 loss function value = 569.0154395567058\n",
      "0.06548223797801928\n",
      "iteration 84 loss function value = 569.0143925911802\n",
      "0.06616422910161751\n",
      "iteration 85 loss function value = 569.0133647716505\n",
      "0.0668542896612639\n",
      "iteration 86 loss function value = 569.0123557537414\n",
      "0.06755253749113785\n",
      "iteration 87 loss function value = 569.0113651992469\n",
      "0.06825909261990072\n",
      "iteration 88 loss function value = 569.0103927760229\n",
      "0.06897407732193976\n",
      "iteration 89 loss function value = 569.0094381578805\n",
      "0.06969761617006304\n",
      "iteration 90 loss function value = 569.0085010244813\n",
      "0.07042983608969466\n",
      "iteration 91 loss function value = 569.0075810612346\n",
      "0.0711708664146157\n",
      "iteration 92 loss function value = 569.0066779591946\n",
      "0.0719208389443063\n",
      "iteration 93 loss function value = 569.00579141496\n",
      "0.07267988800293816\n",
      "iteration 94 loss function value = 569.0049211305756\n",
      "0.07344815050007819\n",
      "iteration 95 loss function value = 569.0040668134347\n",
      "0.07422576599315808\n",
      "iteration 96 loss function value = 569.0032281761834\n",
      "0.07501287675176722\n",
      "iteration 97 loss function value = 569.0024049366256\n",
      "0.0758096278238385\n",
      "iteration 98 loss function value = 569.0015968176317\n",
      "0.07661616710378617\n",
      "iteration 99 loss function value = 569.0008035470462\n",
      "SGD accuracy: 0.6622807017543859\n",
      "SGD recall: 0.5147058823529411\n",
      "SGD precision: 0.65625\n",
      "SGD F measure: 0.576923076923077\n"
     ]
    }
   ],
   "source": [
    "model2 = logit(X2_train, y2_train ,add_intercept = True)\n",
    "loss2, betas2 = model2.fit(method = \"SGD\", verbose = True, max_iter = 100, lr = 0.05, stop_condition = 0.00000001)\n",
    "print(\"SGD accuracy:\", model2.accuracy(model2.X, model2.y))\n",
    "print(\"SGD recall:\", model2.recall(model2.X, model2.y))\n",
    "print(\"SGD precision:\", model2.precision(model2.X, model2.y))\n",
    "print(\"SGD F measure:\", model2.f_measure(model2.X, model2.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-592ce9a509a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_intercept\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloss3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbetas3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"IRLS\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_condition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"IRLS accuracy:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"IRLS recall:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"IRLS precision:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-8601b6b85fba>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, stop_condition, lr, method, verbose, max_iter)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m \u001b[1;33m@\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m                 \u001b[1;31m# T x 1 wector of transformed response variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m                 \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m@\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m                 \u001b[1;31m# updating beta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36minv\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mmarz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->D'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->d'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m     \u001b[0mainv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mmarz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Singular matrix\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "model3 = logit(X1_train, y1_train, add_intercept = True)\n",
    "loss3, betas3 = model3.fit(method = \"IRLS\", verbose = True, max_iter = 10, lr = 0.01, stop_condition = 0.0001)\n",
    "print(\"IRLS accuracy:\", model3.accuracy(model3.X, model3.y))\n",
    "print(\"IRLS recall:\", model3.recall(model3.X, model3.y))\n",
    "print(\"IRLS precision:\", model3.precision(model3.X, model3.y))\n",
    "print(\"IRLS F measure:\", model3.f_measure(model3.X, model3.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., 60., 70., ...,  0.,  1.,  0.],\n",
       "       [ 1., 50., 55., ...,  0.,  0.,  0.],\n",
       "       [ 1., 50., 60., ...,  1.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 1., 50., 50., ...,  0.,  0.,  1.],\n",
       "       [ 1., 53., 60., ...,  0.,  1.,  0.],\n",
       "       [ 1., 70., 65., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 loss function value = -0.0\n",
      "iteration 2 loss function value = -0.0\n",
      "iteration 3 loss function value = -0.0\n",
      "iteration 4 loss function value = -0.0\n",
      "iteration 5 loss function value = -0.0\n",
      "iteration 6 loss function value = -0.0\n",
      "iteration 7 loss function value = -0.0\n",
      "iteration 8 loss function value = -0.0\n",
      "iteration 9 loss function value = -0.0\n",
      "IRLS accuracy: 0.09676724137931035\n",
      "IRLS recall: 1.0\n",
      "IRLS precision: 0.09618287686003882\n",
      "IRLS F measure: 0.17548691717489673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mmarz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: overflow encountered in exp\n",
      "c:\\users\\mmarz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:65: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "model3 = logit(X1_train, y1_train, add_intercept = True)\n",
    "loss3, betas3 = model3.fit(method = \"IRLS2\", verbose = True, max_iter = 10, lr = 0.01, stop_condition = 0.0001)\n",
    "print(\"IRLS accuracy:\", model3.accuracy(model3.X, model3.y))\n",
    "print(\"IRLS recall:\", model3.recall(model3.X, model3.y))\n",
    "print(\"IRLS precision:\", model3.precision(model3.X, model3.y))\n",
    "print(\"IRLS F measure:\", model3.f_measure(model3.X, model3.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2) (112, 3)\n",
      "0.004244812608449361\n",
      "iteration 1 loss function value = -263.3469319750888\n",
      "(3, 2) (112, 3)\n",
      "0.004226889136442209\n",
      "iteration 2 loss function value = -264.2262490049628\n",
      "(3, 2) (112, 3)\n",
      "0.004209116464545004\n",
      "iteration 3 loss function value = -265.1072253545466\n",
      "(3, 2) (112, 3)\n",
      "0.004191492697569943\n",
      "iteration 4 loss function value = -265.9898434061578\n",
      "(3, 2) (112, 3)\n",
      "0.004174015971953638\n",
      "iteration 5 loss function value = -266.8740856882573\n",
      "(3, 2) (112, 3)\n",
      "0.004156684455101067\n",
      "iteration 6 loss function value = -267.75993487525653\n",
      "(3, 2) (112, 3)\n",
      "0.004139496344743989\n",
      "iteration 7 loss function value = -268.6473737872956\n",
      "(3, 2) (112, 3)\n",
      "0.004122449868317153\n",
      "iteration 8 loss function value = -269.5363853899944\n",
      "(3, 2) (112, 3)\n",
      "0.0041055432823481775\n",
      "iteration 9 loss function value = -270.426952794176\n",
      "(3, 2) (112, 3)\n",
      "0.004088774871863595\n",
      "iteration 10 loss function value = -271.31905925556396\n",
      "(3, 2) (112, 3)\n",
      "0.004072142949808557\n",
      "iteration 11 loss function value = -272.21268817445394\n",
      "(3, 2) (112, 3)\n",
      "0.00405564585648125\n",
      "iteration 12 loss function value = -273.10782309536063\n",
      "(3, 2) (112, 3)\n",
      "0.004039281958980655\n",
      "iteration 13 loss function value = -274.0044477066402\n",
      "(3, 2) (112, 3)\n",
      "0.00402304965066775\n",
      "iteration 14 loss function value = -274.90254584008983\n",
      "(3, 2) (112, 3)\n",
      "0.004006947350639889\n",
      "iteration 15 loss function value = -275.80210147052463\n",
      "(3, 2) (112, 3)\n",
      "0.00399097350321718\n",
      "iteration 16 loss function value = -276.70309871533135\n",
      "(3, 2) (112, 3)\n",
      "0.003975126577441451\n",
      "iteration 17 loss function value = -277.60552183400307\n",
      "(3, 2) (112, 3)\n",
      "0.0039594050665875844\n",
      "iteration 18 loss function value = -278.5093552276511\n",
      "(3, 2) (112, 3)\n",
      "0.0039438074876853545\n",
      "iteration 19 loss function value = -279.4145834384982\n",
      "(3, 2) (112, 3)\n",
      "0.003928332381053695\n",
      "iteration 20 loss function value = -280.3211911493525\n",
      "(3, 2) (112, 3)\n",
      "0.003912978309844735\n",
      "iteration 21 loss function value = -281.22916318306216\n",
      "(3, 2) (112, 3)\n",
      "0.0038977438595993\n",
      "iteration 22 loss function value = -282.1384845019525\n",
      "(3, 2) (112, 3)\n",
      "0.003882627637812873\n",
      "iteration 23 loss function value = -283.0491402072462\n",
      "(3, 2) (112, 3)\n",
      "0.0038676282735107133\n",
      "iteration 24 loss function value = -283.9611155384658\n",
      "(3, 2) (112, 3)\n",
      "0.0038527444168336973\n",
      "iteration 25 loss function value = -284.8743958728211\n",
      "(3, 2) (112, 3)\n",
      "0.0038379747386332406\n",
      "iteration 26 loss function value = -285.78896672458023\n",
      "(3, 2) (112, 3)\n",
      "0.0038233179300757284\n",
      "iteration 27 loss function value = -286.7048137444265\n",
      "(3, 2) (112, 3)\n",
      "0.0038087727022557603\n",
      "iteration 28 loss function value = -287.62192271880014\n",
      "(3, 2) (112, 3)\n",
      "0.003794337785818304\n",
      "iteration 29 loss function value = -288.54027956922704\n",
      "(3, 2) (112, 3)\n",
      "0.0037800119305895267\n",
      "iteration 30 loss function value = -289.4598703516337\n",
      "(3, 2) (112, 3)\n",
      "0.0037657939052156268\n",
      "iteration 31 loss function value = -290.38068125564956\n",
      "(3, 2) (112, 3)\n",
      "0.003751682496810455\n",
      "iteration 32 loss function value = -291.3026986038976\n",
      "(3, 2) (112, 3)\n",
      "0.0037376765106099364\n",
      "iteration 33 loss function value = -292.2259088512727\n",
      "(3, 2) (112, 3)\n",
      "0.0037237747696353\n",
      "iteration 34 loss function value = -293.1502985842091\n",
      "(3, 2) (112, 3)\n",
      "0.0037099761143637978\n",
      "iteration 35 loss function value = -294.07585451993754\n",
      "(3, 2) (112, 3)\n",
      "0.003696279402404778\n",
      "iteration 36 loss function value = -295.00256350573164\n",
      "(3, 2) (112, 3)\n",
      "0.003682683508186575\n",
      "iteration 37 loss function value = -295.93041251814566\n",
      "(3, 2) (112, 3)\n",
      "0.0036691873226459885\n",
      "iteration 38 loss function value = -296.85938866224126\n",
      "(3, 2) (112, 3)\n",
      "0.0036557897529284305\n",
      "iteration 39 loss function value = -297.7894791708079\n",
      "GD accuracy: 0.10714285714285714\n",
      "GD recall: 0.5283018867924528\n",
      "GD precision: 0.5283018867924528\n",
      "GD F measure: 0.5283018867924528\n"
     ]
    }
   ],
   "source": [
    "#np.seterr(divide='ignore', invalid='ignore')\n",
    "model_multi_1 = logit(X_multi_train, Y_multi_train ,add_intercept = True)\n",
    "loss_multi_1, betas_multi_1 = model_multi_1.fit(method = \"GD\", verbose = True, max_iter = 40, lr = 0.005, stop_condition = 0.00001 )\n",
    "print(\"GD accuracy:\", model_multi_1.accuracy(model_multi_1.X, model_multi_1.y))\n",
    "print(\"GD recall:\", model_multi_1.recall(model_multi_1.X, model_multi_1.y))\n",
    "print(\"GD precision:\", model_multi_1.precision(model_multi_1.X, model_multi_1.y))\n",
    "print(\"GD F measure:\", model_multi_1.f_measure(model_multi_1.X, model_multi_1.y))\n",
    "\n",
    "#print(betas1[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00804212519196098\n",
      "iteration 1 loss function value = 268.8023273107559\n",
      "0.004529089670992998\n",
      "iteration 2 loss function value = 278.10332740716893\n",
      "0.002324101306026164\n",
      "iteration 3 loss function value = 287.9303580854822\n",
      "0.001408790515726595\n",
      "iteration 4 loss function value = 299.9770255390786\n",
      "0.001719435736246789\n",
      "iteration 5 loss function value = 315.8695593564507\n",
      "0.002015141619160097\n",
      "iteration 6 loss function value = 334.87921210321304\n",
      "0.002222443230319943\n",
      "iteration 7 loss function value = 355.8502264416003\n",
      "0.0023199370332149913\n",
      "iteration 8 loss function value = 377.76818270800186\n",
      "0.002328100914976225\n",
      "iteration 9 loss function value = 399.93112900849906\n",
      "0.002280330391739303\n",
      "iteration 10 loss function value = 421.9174387745422\n",
      "0.002204243754711037\n",
      "iteration 11 loss function value = 443.49803544160875\n",
      "0.0021173798769184727\n",
      "iteration 12 loss function value = 464.5603643749671\n",
      "0.0020293312310508353\n",
      "iteration 13 loss function value = 485.05788608736543\n",
      "0.001944774393991593\n",
      "iteration 14 loss function value = 504.9801240518581\n",
      "0.001865678595639745\n",
      "iteration 15 loss function value = 524.3357878110749\n",
      "0.0017926237184476355\n",
      "iteration 16 loss function value = 543.1434461640007\n",
      "0.0017255197495239286\n",
      "iteration 17 loss function value = 561.4263905644059\n",
      "0.0016639813059465918\n",
      "iteration 18 loss function value = 579.2097996100349\n",
      "0.0016075162011482632\n",
      "iteration 19 loss function value = 596.5191731995237\n",
      "0.0015556169374755634\n",
      "iteration 20 loss function value = 613.3794757338835\n",
      "0.0015078024643048686\n",
      "iteration 21 loss function value = 629.8146809471419\n",
      "0.0014636348792127288\n",
      "iteration 22 loss function value = 645.8475471525954\n",
      "0.0014227238098859791\n",
      "iteration 23 loss function value = 661.4995257794594\n",
      "0.001384725008346296\n",
      "iteration 24 loss function value = 676.7907470661471\n",
      "0.001349336476203221\n",
      "iteration 25 loss function value = 691.7400499063317\n",
      "0.0013162937775179318\n",
      "iteration 26 loss function value = 706.365036163977\n",
      "0.0012853653371275916\n",
      "iteration 27 loss function value = 720.6821375976834\n",
      "0.0012563480810452966\n",
      "iteration 28 loss function value = 734.7066882156752\n",
      "0.001229063551964561\n",
      "iteration 29 loss function value = 748.4529977289794\n",
      "0.0012033545228632963\n",
      "iteration 30 loss function value = 761.9344235244492\n",
      "0.0011790820807088187\n",
      "iteration 31 loss function value = 775.1634396714251\n",
      "0.0011561231316245166\n",
      "iteration 32 loss function value = 788.1517021599665\n",
      "0.0011343682734416608\n",
      "iteration 33 loss function value = 800.91010999778\n",
      "0.0011137199833953916\n",
      "iteration 34 loss function value = 813.4488620606794\n",
      "0.0010940910736690615\n",
      "iteration 35 loss function value = 825.7775097560389\n",
      "0.0010754033734521322\n",
      "iteration 36 loss function value = 837.9050056569813\n",
      "0.0010575866020956688\n",
      "iteration 37 loss function value = 849.8397483205174\n",
      "0.0010405774033991374\n",
      "iteration 38 loss function value = 861.5896235306624\n",
      "0.0010243185158292168\n",
      "iteration 39 loss function value = 873.1620422177853\n",
      "0.0010087580575779134\n",
      "iteration 40 loss function value = 884.563975304367\n",
      "0.0009938489088149476\n",
      "iteration 41 loss function value = 895.8019857195161\n",
      "0.0009795481763975814\n",
      "iteration 42 loss function value = 906.8822578126512\n",
      "0.0009658167287009847\n",
      "iteration 43 loss function value = 917.810624382592\n",
      "0.0009526187902413905\n",
      "iteration 44 loss function value = 928.5925915233738\n",
      "0.0009399215874233743\n",
      "iteration 45 loss function value = 939.233361472747\n",
      "0.000927695038121098\n",
      "iteration 46 loss function value = 949.7378536345851\n",
      "0.0009159114789452639\n",
      "iteration 47 loss function value = 960.1107239321275\n",
      "0.0009045454250020269\n",
      "iteration 48 loss function value = 970.3563826356027\n",
      "0.0008935733577418217\n",
      "iteration 49 loss function value = 980.4790107953008\n",
      "0.0008829735371551354\n",
      "iteration 50 loss function value = 990.4825753995826\n",
      "0.0008727258351302856\n",
      "iteration 51 loss function value = 1000.3708433666601\n",
      "0.0008628115872485802\n",
      "iteration 52 loss function value = 1010.147394469382\n",
      "0.000853213460685582\n",
      "iteration 53 loss function value = 1019.815633283009\n",
      "0.000843915336215982\n",
      "iteration 54 loss function value = 1029.3788002383371\n",
      "0.000834902202595956\n",
      "iteration 55 loss function value = 1038.8399818547543\n",
      "0.0008261600618355814\n",
      "iteration 56 loss function value = 1048.2021202211952\n",
      "0.0008176758440734822\n",
      "iteration 57 loss function value = 1057.4680217871512\n",
      "0.0008094373309354591\n",
      "iteration 58 loss function value = 1066.640365519881\n",
      "0.0008014330864051855\n",
      "iteration 59 loss function value = 1075.7217104794586\n",
      "0.0007936523943625103\n",
      "iteration 60 loss function value = 1084.714502858401\n",
      "0.000786085202047022\n",
      "iteration 61 loss function value = 1093.6210825286657\n",
      "0.0007787220688026513\n",
      "iteration 62 loss function value = 1102.4436891350474\n",
      "0.0007715541195323568\n",
      "iteration 63 loss function value = 1111.1844677705858\n",
      "0.0007645730023671408\n",
      "iteration 64 loss function value = 1119.8454742665901\n",
      "0.0007577708501069375\n",
      "iteration 65 loss function value = 1128.428680127018\n",
      "0.0007511402450474223\n",
      "iteration 66 loss function value = 1136.935977134346\n",
      "0.0007446741868495552\n",
      "iteration 67 loss function value = 1145.3691816522519\n",
      "0.0007383660631472236\n",
      "iteration 68 loss function value = 1153.7300386473166\n",
      "0.0007322096226237734\n",
      "iteration 69 loss function value = 1162.020225451388\n",
      "0.0007261989503191452\n",
      "iteration 70 loss function value = 1170.2413552831747\n",
      "0.0007203284449517063\n",
      "iteration 71 loss function value = 1178.394980547413\n",
      "0.0007145927980668844\n",
      "iteration 72 loss function value = 1186.482595926907\n",
      "0.0007089869748415806\n",
      "iteration 73 loss function value = 1194.5056412833574\n",
      "0.0007035061963916085\n",
      "iteration 74 loss function value = 1202.4655043797952\n",
      "0.0006981459234476191\n",
      "iteration 75 loss function value = 1210.3635234375884\n",
      "0.0006929018412747905\n",
      "iteration 76 loss function value = 1218.2009895401816\n",
      "0.0006877698457290135\n",
      "iteration 77 loss function value = 1225.979148892739\n",
      "0.0006827460303479066\n",
      "iteration 78 loss function value = 1233.6992049495543\n",
      "0.0006778266743897746\n",
      "iteration 79 loss function value = 1241.3623204166245\n",
      "0.0006730082317384648\n",
      "iteration 80 loss function value = 1248.9696191389755\n",
      "0.0006682873206018455\n",
      "iteration 81 loss function value = 1256.5221878796615\n",
      "0.0006636607139388586\n",
      "iteration 82 loss function value = 1264.0210779983086\n",
      "0.0006591253305534653\n",
      "iteration 83 loss function value = 1271.4673070353101\n",
      "0.0006546782268040867\n",
      "iteration 84 loss function value = 1278.861860208268\n",
      "0.0006503165888768985\n",
      "iteration 85 loss function value = 1286.2056918261555\n",
      "0.0006460377255802914\n",
      "iteration 86 loss function value = 1293.499726626253\n",
      "0.0006418390616180546\n",
      "iteration 87 loss function value = 1300.744861039505\n",
      "0.0006377181313057313\n",
      "iteration 88 loss function value = 1307.941964387946\n",
      "0.000633672572695499\n",
      "iteration 89 loss function value = 1315.0918800192098\n",
      "0.0006297001220791532\n",
      "iteration 90 loss function value = 1322.1954263810978\n",
      "0.0006257986088398192\n",
      "iteration 91 loss function value = 1329.253398041307\n",
      "0.0006219659506284817\n",
      "iteration 92 loss function value = 1336.2665666542239\n",
      "0.0006182001488402849\n",
      "iteration 93 loss function value = 1343.2356818788323\n",
      "0.000614499284368808\n",
      "iteration 94 loss function value = 1350.1614722503068\n",
      "0.0006108615136193792\n",
      "iteration 95 loss function value = 1357.0446460080361\n",
      "0.0006072850647620361\n",
      "iteration 96 loss function value = 1363.8858918835194\n",
      "0.0006037682342084906\n",
      "iteration 97 loss function value = 1370.6858798486332\n",
      "0.0006003093832963116\n",
      "iteration 98 loss function value = 1377.4452618287987\n",
      "0.0005969069351671344\n",
      "iteration 99 loss function value = 1384.1646723806662\n",
      "SGD accuracy: 0.30357142857142855\n",
      "SGD recall: 0.5894736842105263\n",
      "SGD precision: 0.5894736842105263\n",
      "SGD F measure: 0.5894736842105263\n"
     ]
    }
   ],
   "source": [
    "model_multi_2 = logit(X_multi_train, Y_multi_train ,add_intercept = True)\n",
    "loss_multi_2, betas_multi_2 = model_multi_2.fit(method = \"SGD\", verbose = True, max_iter = 100, lr = 0.01, stop_condition = 0.00000001)\n",
    "print(\"SGD accuracy:\", model_multi_2.accuracy(model_multi_2.X, model_multi_2.y))\n",
    "print(\"SGD recall:\", model_multi_2.recall(model_multi_2.X, model_multi_2.y))\n",
    "print(\"SGD precision:\", model_multi_2.precision(model_multi_1.X, model_multi_1.y))\n",
    "print(\"SGD F measure:\", model_multi_2.f_measure(model_multi_1.X, model_multi_1.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09816091954022989"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09028256374913853"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.12697804,  1.8836361 ,  0.53816435, -1.15908311]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas2[-1].reshape(1,-1)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9038793103448276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.92093266e-12, -1.88008283e-12, -1.86066965e-12,\n",
       "        -1.73261110e-12, -1.86054453e-12, -1.80848485e-12,\n",
       "        -1.23179421e-14, -1.14656623e-07, -2.63900412e-16,\n",
       "         1.18071756e-16, -7.73609476e-16, -1.31709861e-14,\n",
       "        -1.67427823e-15, -9.21501045e-16,  1.63446256e-15,\n",
       "        -7.53259397e-15, -2.06003973e-15]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver = \"lbfgs\", fit_intercept=True, max_iter=1000)\n",
    "model.fit(X1_train, y1_train)\n",
    "print(model.score(X1_train, y1_train))\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.229942\n",
      "         Iterations 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9144396551724138"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.discrete.discrete_model import Logit\n",
    "\n",
    "#X = df.drop(\"chd\", axis = 1, inplace=False)\n",
    "#y = df.chd\n",
    "\n",
    "model = Logit(endog = y1_train, exog = X1_train)\n",
    "res = model.fit()\n",
    "np.sum((res.predict(X1_train) > 0.5) == y1_train) / X1_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 4)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize = (8,6))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "sequence_containing_x_vals1 = [i[0][0] for i in betas1]\n",
    "sequence_containing_y_vals1 = [i[1][0] for i in betas1]\n",
    "sequence_containing_z_vals1 = loss1\n",
    "\n",
    "sequence_containing_x_vals2 = [i[0][0] for i in betas2]\n",
    "sequence_containing_y_vals2 = [i[1][0] for i in betas2]\n",
    "sequence_containing_z_vals2 = loss2\n",
    "\n",
    "\n",
    "ax.scatter(sequence_containing_x_vals2, sequence_containing_y_vals2, sequence_containing_z_vals2, marker = \"o\",\n",
    "          color = \"blue\", s = 5, alpha =0.3)\n",
    "ax.scatter(sequence_containing_x_vals1, sequence_containing_y_vals1, sequence_containing_z_vals1, marker = \"o\",\n",
    "          color = \"green\", s = 40)\n",
    "\n",
    "plt.title(\"3D statter plot\")\n",
    "ax.set_xlabel(\"Beta 1\")\n",
    "ax.set_ylabel(\"Beta 2\")\n",
    "ax.set_zlabel(\"Values of loss function\")\n",
    "plt.legend([\"GD\", \"SGD\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
